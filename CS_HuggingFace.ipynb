{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3b8f64-91ee-49b0-bae7-b2c5198995b4",
   "metadata": {},
   "source": [
    "# Hugging Face Cheat Sheet\n",
    "\n",
    "<!--- Start of badges -->
<!-- Badges: python,raspberrypi,api,3d_printing,electronics,tfl -->

<p align="left">
<img alt="3D Printing" src="https://img.shields.io/badge/-3D Printing-FF5722.svg?logo=data:image/svg+xml;base64,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&style=flat-square" />
 <img alt="Api" src="https://img.shields.io/badge/-API-007ACC.svg?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBMaWNlbnNlOiBNSVQuIE1hZGUgYnkgR2l0bGFiOiBodHRwczovL2dpdGxhYi5jb20vZ2l0bGFiLW9yZy9naXRsYWItc3Zncz9yZWY9aWNvbmR1Y2suY29tIC0tPgo8c3ZnIHdpZHRoPSI4MDBweCIgaGVpZ2h0PSI4MDBweCIgdmlld0JveD0iMCAwIDE2IDE2IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNOS4yNTMgMi43ODJsLTEuMTEtLjI3M2E0LjAyMSA0LjAyMSAwIDAwLS41OTMgMS4wMjVsLjc5LjgyNWEzLjAwNSAzLjAwNSAwIDAwMCAxLjI4MmwtLjc5LjgyNWMuMTQ3LjM3Mi4zNDguNzE3LjU5MyAxLjAyNWwxLjExLS4yNzNjLjMxNS4yODguNjkzLjUwOSAxLjEwOS42NDFsLjMxOCAxLjA5N2EzLjk5OSAzLjk5OSAwIDAwMS4xODUgMGwuMzE4LTEuMDk3YTIuOTg1IDIuOTg1IDAgMDAxLjExLS42NDFsMS4xMS4yNzNjLjI0NS0uMzA4LjQ0Ni0uNjUzLjU5My0xLjAyNWwtLjc5Mi0uODI1YTMuMDA1IDMuMDA1IDAgMDAwLTEuMjgybC43OTItLjgyNWE0LjAyMSA0LjAyMSAwIDAwLS41OTMtMS4wMjVsLTEuMTEuMjczYTIuOTg1IDIuOTg1IDAgMDAtMS4xMS0uNjQxbC0uMzE4LTEuMDk3YTMuOTg1IDMuOTg1IDAgMDAtMS4xODUgMGwtLjMxOCAxLjA5N2EyLjk4MiAyLjk4MiAwIDAwLTEuMTEuNjQxek0xMi4yNzMgNWExIDEgMCAxMS0yIDAgMSAxIDAgMDEyIDB6IiBmaWxsPSIjZmZmZmZmIi8+CiAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik02LjkxNSA2LjI2NEE0LjU0IDQuNTQgMCAwMDUuNyA2LjA0bC0uNDQ1LjkxOGEzLjU4OCAzLjU4OCAwIDAwLTEuMzY0LjM3bC0uODUtLjU2NGMtLjM0NS4yMjYtLjY2LjQ5OS0uOTMyLjgxbC40NC45MmMtLjI3LjM5MS0uNDYyLjgzMy0uNTYgMS4yOThsLS45NzEuMzEyYTQuNTEgNC41MSAwIDAwLjA1MSAxLjIzM2wuOTk1LjIzYy4xMzYuNDU2LjM2My44OC42NjUgMS4yNDhsLS4zNjIuOTU0Yy4yOTguMjg3LjYzMi41MzIuOTk2LjcyOGwuOC0uNjMzYTMuNTggMy41OCAwIDAwMS4zOS4yNTdsLjUyMS44NzdjLjQwOS0uMDUzLjgxLS4xNjIgMS4xOS0uMzI0bC4wMDQtMS4wMmEzLjU4IDMuNTggMCAwMDEuMDY3LS45MjdsMS4wMS4xNGMuMjE1LS4zNTMuMzgtLjczNS40OS0xLjEzM2wtLjc5Ni0uNjM5YTMuNjEgMy42MSAwIDAwLS4wNTgtMS40MTNsLjczOS0uNzAxYTQuNTA4IDQuNTA4IDAgMDAtLjU4MS0xLjA5bC0uOTk1LjIyNGEzLjU4OCAzLjU4OCAwIDAwLTEuMTQxLS44MzRsLS4wODgtMS4wMTd6bS0xLjE1IDIuODJhMS40NzcgMS40NzcgMCAxMS0uNTM4IDIuOTA0IDEuNDc3IDEuNDc3IDAgMDEuNTM5LTIuOTA1eiIgZmlsbD0iI2ZmZmZmZiIvPgo8L3N2Zz4=&style=flat-square" />
 <img alt="Electronics" src="https://img.shields.io/badge/-Electronics-2E7D32.svg?style=flat-square&logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0naXNvLTg4NTktMSc/Pgo8IS0tIExpY2Vuc2U6IENDMC4gTWFkZSBieSBTVkcgUmVwbzogaHR0cHM6Ly93d3cuc3ZncmVwby5jb20vc3ZnLzE3MjE4MC9jZWxsIC0tPgo8c3ZnIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgMjk4IDI5OCIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI5OCAyOTgiPgogIDxnIGZpbGw9IiNmZmZmZmYiPgogICAgPHBvbHlnb24gcG9pbnRzPSIyOTgsMTQxIDE4MiwxNDEgMTgyLDExNiAxNjYsMTE2IDE2NiwxODIgMTgyLDE4MiAxODIsMTU3IDI5OCwxNTcgICIvPgogICAgPHBvbHlnb24gcG9pbnRzPSIxMTYsMTQxIDAsMTQxIDAsMTU3IDExNiwxNTcgMTE2LDIxNSAxMzIsMjE1IDEzMiw4MyAxMTYsODMgICIgLz4KICA8L2c+Cjwvc3ZnPgo=" />
 <img alt="Python" src="https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white&style=flat-square" />
 <img alt="Raspberrypi" src="https://img.shields.io/badge/-Raspberry%20Pi-C51A4A?logo=raspberry-pi&logoColor=white&style=flat-square" />
 <img alt="Tfl" src="https://img.shields.io/badge/-TfL-000f9f.svg?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjRweCIgaGVpZ2h0PSIyNHB4IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHJvbGU9ImltZyIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDx0aXRsZT5UcmFuc3BvcnQgZm9yIExvbmRvbiBpY29uPC90aXRsZT4KICAgIDxwYXRoIGZpbGw9IiNmZmZmZmYiIGQ9Ik0xMiAyLjI1YTkuNzMgOS43MyAwIDAgMC05LjQ5IDcuNUgwdjQuNWgyLjUxYTkuNzMgOS43MyAwIDAgMCA5LjQ5IDcuNWM0LjYyIDAgOC40OC0zLjIgOS40OS03LjVIMjR2LTQuNWgtMi41MUE5LjczIDkuNzMgMCAwIDAgMTIgMi4yNXpNMTIgNmMyLjUgMCA0LjY2IDEuNTYgNS41NiAzLjc1SDYuNDRBNi4wMiA2LjAyIDAgMCAxIDEyIDZ6bS01LjU2IDguMjVoMTEuMTJBNi4wMiA2LjAyIDAgMCAxIDEyIDE4YTYuMDIgNi4wMiAwIDAgMS01LjU2LTMuNzVaIiAvPgo8L3N2Zz4=&logoColor=white&style=flat-square" />
</p>

<!--- End of badges -->\n",
    "\n",
    "This notebook serves as a comprehensive cheat sheet for working with the Hugging Face Transformers library, based on material from the ['Generative AI Engineering and Fine-Tuning Transformers'](https://www.coursera.org/learn/generative-ai-engineering-and-fine-tuning-transformers/home/welcome) and ['Generative AI Advance Fine-Tuning for LLMs'](https://www.coursera.org/learn/generative-ai-advanced-fine-tuning-for-llms/home/welcome) modules by IBM on Coursera.\n",
    "\n",
    "The notebook covers the following concepts:\n",
    "\n",
    "- **Datasets:** Demonstrates loading and utilizing various datasets commonly used in NLP tasks, such as WikiText, SNLI (Stanford Natural Language Inference), Yelp reviews, Open Assistant, IMDB, CodeAlpaca, and Synthetic instruct GPT-J pairwise.\n",
    "- **Building Blocks:**\n",
    "  - **Untrained Models:** Shows how to initialize and configure untrained models.\n",
    "  - **Pre-trained Models:** Provides examples of using pre-trained models both with and without the pipeline abstraction. This includes tasks like text classification/sentiment analysis (DistilBERT), text generation (GPT-2, OPT), text translation (T5), and fill-mask (BERT).\n",
    "- **Pre-training:** Focuses on self-supervised pre-training using a BERT model on the WikiText dataset for Masked Language Modelling. It covers data preparation, model definition, DataCollator usage, model training, evaluation (perplexity), and making inferences.\n",
    "- **Fine-tuning for Sequence Classification:**\n",
    "  - Supervised Full Fine-tuning: Illustrates full fine-tuning of BERT models on datasets like SNLI and Yelp for sequence classification tasks, including data preparation, model loading, fine-tuning, and accuracy evaluation.\n",
    "  - Supervised LoRA Fine-tuning: Introduces Low-Rank Adaptation (LoRA) for fine-tuning, demonstrated with a DistilBERT model on the IMDB dataset, covering data preparation, model loading, fine-tuning, and evaluation of training loss and validation accuracy.\n",
    "- **Instruction-based Fine-tuning of Generative Models (with SFTTrainer):**\n",
    "  - Supervised Full Fine-tuning: Details the fine-tuning of an OPT model using the Open Assistant dataset, covering data preparation, model and tokenizer instantiation, DataCollator for completion, training with SFTTrainer, and evaluation (perplexity and inference).\n",
    "  - Supervised LoRA Fine-tuning: Extends LoRA application to generative models, fine-tuning an OPT model on the CodeAlpaca dataset, including data preparation, DataCollator and pre-processing functions, fine-tuning, evaluation (training loss, inference, BLEU score), and model saving.\n",
    "- **Reward Models:** Shows how to fine-tune a GPT-2 model using LoRA with the Synthetic instruct GPT-J pairwise dataset for reward modeling, covering data preparation, model loading, fine-tuning, evaluation (training loss, inference), and model saving.\n",
    "- **Retrieval Augmented Generation (RAG):** Explains the implementation of a RAG system using a Dense Passage Retrieval (DPR) encoder and an LLM generator. This section covers data preparation, instantiating tokenizers and encoders for context, questions, and the LLM, embedding the context, finding relevant contexts using FAISS, and generating answers with the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf615169-e72d-42db-9e7b-43a438bcca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"background-color: whitesmoke; padding: 10px; padding-left: 30px;\">\n",
       "  <h2>Table of Contents</h2>\n",
       "  <hr>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Datasets\">1. Datasets</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#WikiText\">WikiText</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#SNLI-(Stanford-Natural-Language-Inference)\">SNLI (Stanford Natural Language Inference)</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Yelp-reviews\">Yelp reviews</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Open-assistant\">Open assistant</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#IMDB\">IMDB</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#CodeAlpaca\">CodeAlpaca</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Synthetic-instruct-GPT-J-pairwise\">Synthetic instruct GPT-J pairwise</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Building-blocks\">2. Building blocks</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Untrained-models\">Untrained models</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#BERT\">BERT</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Pre-trained-models\">Pre-trained models</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Without-pipeline\">Without pipeline</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-classification-/-sentiment-analysis-(DistilBERT)\">Text classification / sentiment analysis (DistilBERT)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-generation-(GPT-2)\">Text generation (GPT-2)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fill-mask-(BERT)\">Fill-mask (BERT)</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#With-pipeline\">With pipeline</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-classification-/-sentiment-analysis-(DistilBERT)\">Text classification / sentiment analysis (DistilBERT)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-generation-(GPT-2)\">Text generation (GPT-2)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-generation-(OPT)\">Text generation (OPT)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-translation-(T5)\">Text translation (T5)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fill-mask-(BERT)\">Fill-mask (BERT)</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Pre-training\">3. Pre-training</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Self-supervised-BERT-model-WikiText-dataset-Masked-Language-Modelling\">Self-supervised - BERT model - WikiText dataset - Masked Language Modelling</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Define-the-model\">Define the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#DataCollator\">DataCollator</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Train-the-model\">Train the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Perplexity\">Perplexity</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Make-inference\">Make inference</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Fine-tuning-for-sequence-classification\">4. Fine-tuning for sequence classification</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-full-fine-tuning-BERT-model-SNLI-dataset\">Supervised full fine-tuning - BERT model - SNLI dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(bert-base-uncased)\">Load pre-trained model (bert-base-uncased)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Accuracy\">Accuracy</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-full-fine-tuning-BERT-model-Yelp-dataset\">Supervised full fine-tuning - BERT model - Yelp dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(bert-base-cased)\">Load pre-trained model (bert-base-cased)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Accuracy\">Accuracy</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-LoRA-fine-tuning-DistilBERT-model-IMDB-dataset\">Supervised LoRA fine-tuning - DistilBERT model - IMDB dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(distilbert-base-cased)\">Load pre-trained model (distilbert-base-cased)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Training-loss-and-validation-accuracy\">Training loss and validation accuracy</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Instruction-based-fine-tuning-of-generative-models-(w/-SFTTrainer)\">5. Instruction-based fine-tuning of generative models (w/ SFTTrainer)</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-full-fine-tuning-OPT-model-open-assistant-dataset\">Supervised full fine-tuning - OPT model - open assistant dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(opt-350m)\">Load pre-trained model (opt-350m)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#DataCollator\">DataCollator</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Perplexity\">Perplexity</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Make-inference\">Make inference</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-LoRA-fine-tuning-OPT-model-CodeAlpaca-dataset\">Supervised LoRA fine-tuning - OPT model - CodeAlpaca dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(opt-350m)\">Load pre-trained model (opt-350m)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#DataCollator-and-pre-processing-function\">DataCollator and pre-processing function</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Training-loss\">Training loss</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Make-inference\">Make inference</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#BLEU-score\">BLEU score</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Save-model\">Save model</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Reward-models\">6. Reward models</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Supervised-LoRA-fine-tuning-GPT2-model-Synthetic-instruct-GPT-J-pairwise-dataset\">Supervised LoRA fine-tuning - GPT2 model - Synthetic instruct GPT-J pairwise dataset</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Load-pre-trained-model-(gpt2)\">Load pre-trained model (gpt2)</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Fine-tune-the-model\">Fine-tune the model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Evaluate\">Evaluate</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Training-loss\">Training loss</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Make-inference\">Make inference</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Save-model\">Save model</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Retrieval-Augmented-Generation-(RAG)\">7. Retrieval Augmented Generation (RAG)</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Dense-Passage-Retrieval-(DPR)-encoder-&-LLM-generator\">Dense Passage Retrieval (DPR) encoder & LLM generator</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Prepare-the-data\">Prepare the data</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Instantiate-tokenizers-&-encoders-for-the-context,-questions-and-LLM\">Instantiate tokenizers & encoders for the context, questions and LLM</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Embed-the-context\">Embed the context</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Embed-the-question-and-find-relevant-context-using-FAISS\">Embed the question and find relevant context using FAISS</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Generate-answers-using-GPT2-model-with-retrieved-information-as-input\">Generate answers using GPT2 model with retrieved information as input</a></div>\n",
       "  <hr>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import generate_notebook_toc \n",
    "from IPython.display import display, Markdown\n",
    "current_notebook_filename = \"CS_HuggingFace.ipynb\"\n",
    "display(Markdown(generate_notebook_toc.get_html_toc(current_notebook_filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462734f6-6140-49cd-a4d9-ce6a14bfcaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c86f9b-7f46-4ca4-b767-19c83e3bd58e",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a31b7-2dcc-4c95-859b-f1a3d1ecd8e2",
   "metadata": {},
   "source": [
    "### WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6055cc8-76c6-42d2-916b-c4d28a48c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bc6c7-1683-488b-a374-bcefcabb1132",
   "metadata": {},
   "source": [
    "### SNLI (Stanford Natural Language Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6c2c01-27b8-4869-8486-85968c1ded7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "snli_dataset = load_dataset(\"stanfordnlp/snli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed0f19-ca82-4eb6-a5eb-95f6b20e49c2",
   "metadata": {},
   "source": [
    "### Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2067f6-2bf0-498a-973f-5dfee3bdb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "yelp_dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874935c-9788-4db3-b6c2-c7dd6d0b6a06",
   "metadata": {},
   "source": [
    "### Open assistant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b288c246-7d47-4b1e-a4ec-5f5338f24ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "open_assistant_dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eaefdb-c6df-48b6-b932-afb2519f902c",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6302fe-8e9a-4dcb-b27f-fa7d1f3e7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "IMDB_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e435a07-17cb-48c6-9f3d-a3a685a08efe",
   "metadata": {},
   "source": [
    "### CodeAlpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588f0e29-e9a6-4f31-b169-05a654ef89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "code_alpaca_dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d2277-3864-4619-9f42-4cb1fba0b8e1",
   "metadata": {},
   "source": [
    "### Synthetic instruct GPT-J pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5343e69e-179e-45f4-93d5-196602cc5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "synthetic_pairwise_dataset = load_dataset(\"Dahoas/synthetic-instruct-gptj-pairwise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd0c36-ede6-4d28-acfc-5aa6786e1717",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63489b73-0f57-44b0-b266-53347d681faa",
   "metadata": {},
   "source": [
    "### Untrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cac4b3-4351-41db-a4fc-ce4bdd5914e0",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a9f1c2-7989-444b-822f-389cde72178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "vocab_size = 100 #len(tokenizer.get_vocab())\n",
    "\n",
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=vocab_size,  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size=768,  # Set the hidden size\n",
    "    num_hidden_layers=12,  # Set the number of hidden layers\n",
    "    num_attention_heads=12,  # Set the number of attention heads in each attention layer\n",
    "    intermediate_size=3072,  # Specifies the size of the \"intermediate\" (i.e., feed-forward) layer within the transformer\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b25b6-fc76-4d87-ae73-31a9b1af6035",
   "metadata": {},
   "source": [
    "### Pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac19b9d-db94-44ac-b171-68e4d2106b84",
   "metadata": {},
   "source": [
    "### Without pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e83892-89bd-412b-a8fa-e3ce25edfecb",
   "metadata": {},
   "source": [
    "#### Text classification / sentiment analysis (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddda6ea-a6bc-45a9-b9af-ebac6841d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text classification / sentiment analysis (DistilBERT)\"\"\"\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probs = torch.softmax(logits, dim=-1) # Convert logits to probabilities\n",
    "predicted_class = torch.argmax(probs, dim=-1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e335760-3a62-4d1b-9eea-18836c94335d",
   "metadata": {},
   "source": [
    "#### Text generation (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e05c96-3834-4a9d-a99b-f2baa2379fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text generation (GPT-2) \"\"\"\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_output = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31a7f2-7bcd-456e-844a-9d77a7a55209",
   "metadata": {},
   "source": [
    "#### Fill-mask (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e073e-4dde-4179-b388-1b5468cf61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fill-mask (BERT) \"\"\"\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probs = torch.softmax(logits, dim=-1) # Convert logits to probabilities\n",
    "predicted_token_id = torch.argmax(probs[0, mask_token_index, :], dim=-1)    \n",
    "predicted_word = tokenizer.decode(predicted_token_id, skip_special_tokens=True)\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b090c-d128-4eb3-aa33-5fdbb0872a1d",
   "metadata": {},
   "source": [
    "### With pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf12b521-5387-4b3d-87e6-860e7e66fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa780c-ca75-411e-be1f-fbbc9b7a495d",
   "metadata": {},
   "source": [
    "#### Text classification / sentiment analysis (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fca541f-14df-47e6-b946-3e12733a84ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Text classification / sentiment analysis (DistilBERT)\"\"\"\n",
    "\n",
    "prompt = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ac389-0b35-436a-99e5-44d6bac442d2",
   "metadata": {},
   "source": [
    "#### Text generation (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9dae65-5710-4574-8c4e-52277ce3215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, these are the words of a child.\n",
      "\n",
      "\"I want to find you, find you,\" he said.\n",
      "\n",
      "After a while, I got out of my car, got to the door, and started crying.\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "I felt him go away, but he was still there. I said,\n",
      "\n",
      "\"I want to find you, find you.\"\n",
      "\n",
      "He was with me on the way to school.\n",
      "\n",
      "The next thing I remember, I was there.\n",
      "\n",
      "There was a man who was on a road trip. He was in a wheelchair.\n",
      "\n",
      "I said,\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "He was with me on the way to school.\n",
      "\n",
      "I said,\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "He was with me on the way to school.\n",
      "\n",
      "I said,\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "He was with me on the way to school.\n",
      "\n",
      "I said,\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "He was with me on the way to school.\n",
      "\n",
      "I said,\n",
      "\n",
      "\"I want to find you.\"\n",
      "\n",
      "He was with me on the way to school\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Text generation (GPT-2) \"\"\"\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f25d33-aa4b-40bd-a3e1-6382a9d31f77",
   "metadata": {},
   "source": [
    "#### Text generation (OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e65db2-371f-4f91-b6fd-3ba5c476882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in the world\n",
      "\n",
      "Welcome to the world of Serenity.\n",
      "\n",
      "The world of Serenity is filled with wonder, dreams, and love.\n",
      "\n",
      "Serenity is a series of adventures, adventures, and adventures. It is a time of hope, joy, and love.\n",
      "\n",
      "Our stories in this series are created by the wonderful volunteers who have been working on the project for the past seven years.\n",
      "\n",
      "Welcome to Serenity.\n",
      "\n",
      "The world of Serenity is filled with wonder, dreams, and love.\n",
      "\n",
      "Serenity is a series of adventures, adventures, and adventures. It is a time of hope, joy, and love.\n",
      "\n",
      "Our stories in this series are created by the wonderful volunteers who have been working on the project for the past seven years.\n",
      "\n",
      "Welcome to Serenity.\n",
      "\n",
      "The world of Serenity is filled with wonder, dreams, and love.\n",
      "\n",
      "Serenity is a series of adventures, adventures, and adventures. It is a time of hope, joy, and love.\n",
      "\n",
      "Our stories in this series are created by the wonderful volunteers who have been working on the project for the past seven years.\n",
      "\n",
      "Welcome to Serenity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Text generation (OPT - Open Pre-trained Transformers) \"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model,tokenizer=tokenizer)\n",
    "generated_text = generator(prompt, max_length=50)\n",
    "print(generated_text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a12cc-03f0-4542-a6a8-fa3438a565eb",
   "metadata": {},
   "source": [
    "#### Text translation (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67cfb64-00c9-4aca-81c1-8b7972219c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment Ãªtes-vous?\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Text translation (T5) \"\"\"\n",
    "\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d89acf-61c9-4019-9095-f9818ca5019c",
   "metadata": {},
   "source": [
    "#### Fill-mask (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b857bc1e-c97c-42ad-a573-4f74293909f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of france is paris.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fill-mask (BERT) \"\"\"\n",
    "\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "generated_text = fill_mask(prompt)\n",
    "print(generated_text[0]['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9c81f-d477-43c7-aaed-c2668f0f71cb",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17652e15-671e-4c2c-949d-b8297fcd6e8a",
   "metadata": {},
   "source": [
    "### Self-supervised - BERT model - WikiText dataset - Masked Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5d80c-85ea-4db4-b603-3517ca5f1310",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ca28ea7-e32a-4733-b36d-8d34bc66dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['text'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418120988e4742dfb6cc1f2cd4e9aa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174ffc555a7c464ab59c84edd40ad7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5982c558f677439da019337d5186a80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4786bad6816d4c36895797759d38a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys after tokenization: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "wiki_dataset_ = copy.deepcopy(wiki_dataset)\n",
    "wiki_dataset_[\"train\"] = wiki_dataset_[\"train\"].select([i for i in range(1000)])\n",
    "wiki_dataset_[\"test\"] = wiki_dataset_[\"test\"].select([i for i in range(200)])\n",
    "print(f'Original keys: {wiki_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Instantiate tokenizer \"\"\"\n",
    "# Instantiate a tokenizer using the BERT base uncased model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\"\"\" Optional: re-train tokenizer based on custom dataset \"\"\"\n",
    "def batch_iterator(dataset, batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "        \n",
    "## train the tokenizer using dataset\n",
    "tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(wiki_dataset), vocab_size=len(tokenizer.get_vocab()))\n",
    "\n",
    "\"\"\" Tokenize dataset \"\"\"\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    # Apply truncation to limit the maximum sequence length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "# Adds output of of tokenizer to the dataset dictionary\n",
    "wiki_dataset_tokenized = wiki_dataset_.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f'Keys after tokenization: {wiki_dataset_tokenized[\"train\"][0].keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176984c1-c604-410b-b34b-c324c2b7d273",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "749d830a-d45d-4ddc-81d9-e3cc157b856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define BERT model \"\"\"\n",
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size=768,  # Set the hidden size\n",
    "    num_hidden_layers=12,  # Set the number of hidden layers\n",
    "    num_attention_heads=12,  # Set the number of attention heads in each attention layer\n",
    "    intermediate_size=3072,  # Specifies the size of the \"intermediate\" (i.e., feed-forward) layer within the transformer\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8188ef4-a20e-46cc-8053-af8a6b8ad833",
   "metadata": {},
   "source": [
    "#### DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef69a04d-65b2-473d-bef1-336d6686ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, #the datacollator does not tokenize, but needs the tokenizer to know what the special tokens are for [MASK] etc. \n",
    "                                                mlm=True, #Indicates that the data collator should mask tokens\n",
    "                                                mlm_probability=0.15 # Sets the probability with which tokens will be masked\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d446e-6a97-479d-92b1-fc824fd821e3",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136f0408-a587-4054-bdad-e674c93b4c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 09:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.622700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=6.940758463541667, metrics={'train_runtime': 602.8933, 'train_samples_per_second': 4.976, 'train_steps_per_second': 2.488, 'total_flos': 789614438400000.0, 'train_loss': 6.940758463541667, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    overwrite_output_dir=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Set the batch size for training\n",
    "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
    "    # logging_steps = 20\n",
    "    \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=wiki_dataset_tokenized[\"train\"],\n",
    "    eval_dataset=wiki_dataset_tokenized[\"test\"],\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b307768-c5f8-4511-8d20-b7053e851c35",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25196b9-84d8-46f2-9766-ba1a64a9886f",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da30071a-6020-4359-92e9-d7ee7a9dff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1711.01\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565cd02-2bcd-4c06-bf84-6562edf4f903",
   "metadata": {},
   "source": [
    "##### Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d98f2734-a6c0-4ed4-9989-6f6bd11c47a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of france is the.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer = tokenizer)\n",
    "generated_text = fill_mask(prompt)\n",
    "print(generated_text[0]['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ff60f-a741-4685-a7f1-554c69648c07",
   "metadata": {},
   "source": [
    "## Fine-tuning for sequence classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674117a7-3bb3-4582-ab0c-b2c3a1c496e0",
   "metadata": {},
   "source": [
    "### Supervised full fine-tuning - BERT model - SNLI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f53bb8-4976-40aa-94c5-9ee9420491cd",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a16d3975-3196-4e1c-ac7c-f37cf2c020fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['premise', 'hypothesis', 'label'])\n",
      "Keys after tokenization: dict_keys(['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "snli_dataset_ = copy.deepcopy(snli_dataset)\n",
    "snli_dataset_[\"train\"] = snli_dataset_[\"train\"].select([i for i in range(1000)])\n",
    "snli_dataset_[\"test\"] = snli_dataset_[\"test\"].select([i for i in range(200)])\n",
    "snli_dataset_[\"validation\"] = snli_dataset_[\"validation\"].select([i for i in range(200)])\n",
    "print(f'Original keys: {snli_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Instantiate tokenizer \"\"\"\n",
    "# Instantiate a tokenizer using the BERT base uncased model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\"\"\" Tokenize dataset \"\"\"\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    # Apply truncation to limit the maximum sequence length\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "# Adds output of of tokenizer to the dataset dictionary\n",
    "snli_dataset_tokenized = snli_dataset_.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f'Keys after tokenization: {snli_dataset_tokenized[\"train\"][0].keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1567d-f54b-4e92-950e-355f94f27ab9",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (bert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa62e168-507e-45b1-9aa9-01afb99a71cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\"\"\" Define BERT model (from a pre-trained model) \"\"\"\n",
    "num_class_snli=len(set(snli_dataset_[\"train\"]['label']))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_class_snli)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325489b7-a8ae-459e-bcca-ca1fa89e5fe1",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29d1b093-de25-436b-9ac7-3d95adf60e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 08:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.529000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.8276377970377604, metrics={'train_runtime': 514.3016, 'train_samples_per_second': 5.833, 'train_steps_per_second': 2.917, 'total_flos': 789347340288000.0, 'train_loss': 0.8276377970377604, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Set the batch size for training \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=snli_dataset_tokenized[\"train\"],\n",
    "    eval_dataset=snli_dataset_tokenized[\"validation\"],\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2f7bd-4fed-416d-ba66-75b3ff1c801e",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434f6b3-492f-4b3d-b211-040995c03675",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e893ce92-a9ec-484d-b2fa-f35748d58151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.7050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions, labels, _ = trainer.predict(snli_dataset_tokenized[\"test\"])\n",
    "accuracy = accuracy_score(labels, predictions.argmax(-1))\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c6b6f-ecd7-461a-81de-14d115c7a37b",
   "metadata": {},
   "source": [
    "### Supervised full fine-tuning - BERT model - Yelp dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa1fda-50ae-4019-8742-2a69a88cd9f3",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "199bcf0d-7c0a-41b9-99d4-a20452f6edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['label', 'text'])\n",
      "Keys after tokenization: dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "yelp_dataset_ = copy.deepcopy(yelp_dataset)\n",
    "yelp_dataset_[\"train\"] = yelp_dataset[\"train\"].select([i for i in range(1000)])\n",
    "yelp_dataset_[\"test\"] = yelp_dataset[\"test\"].select([i for i in range(200)])\n",
    "print(f'Original keys: {yelp_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Tokenize dataset \"\"\"\n",
    "\n",
    "# Instantiate a tokenizer using the BERT base cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    # Apply truncation to limit the maximum sequence length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "# Adds output of of tokenizer to the dataset dictionary\n",
    "yelp_dataset_tokenized = yelp_dataset_.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename the label column to labels because the model expects the argument to be named labels\n",
    "yelp_dataset_tokenized = yelp_dataset_tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(f'Keys after tokenization: {yelp_dataset_tokenized[\"train\"][0].keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfb9db-df7a-479f-87d2-0f5d81872828",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (bert-base-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff5893d1-2f8c-4baf-9909-093b6f51d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "num_class_yelp=len(set(yelp_dataset_[\"train\"]['label']))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=num_class_yelp)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390e0cd-2c5a-4cd7-b26f-3f91b1bef360",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bcc7bd4-c3a6-4257-a4e8-531d5f3323ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 08:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=1.703685546875, metrics={'train_runtime': 515.6232, 'train_samples_per_second': 5.818, 'train_steps_per_second': 2.909, 'total_flos': 789354427392000.0, 'train_loss': 1.703685546875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "LR = 5e-4\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Set the batch size for training \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=yelp_dataset_tokenized[\"train\"],\n",
    "    eval_dataset=yelp_dataset_tokenized[\"test\"],\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12400a83-dfd4-4095-8f28-655ef671ffb6",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1e195-2752-4eec-9309-642e0db7ae34",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6bb7b12-6d50-4851-9416-b9e12e94aee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.1800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions, labels, *_ = trainer.predict(yelp_dataset_tokenized[\"test\"])\n",
    "accuracy = accuracy_score(labels, predictions.argmax(-1))\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988e839-66d5-4fa5-b923-26781913e26f",
   "metadata": {},
   "source": [
    "### Supervised LoRA fine-tuning - DistilBERT model - IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76b02d-6e62-43d6-8cc3-1423cc166d79",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dbfb33e-944b-43d0-934d-574c90b5d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['text', 'label'])\n",
      "Keys after tokenization: dict_keys(['labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "IMDB_dataset_ = copy.deepcopy(IMDB_dataset)\n",
    "IMDB_dataset_[\"train\"] = IMDB_dataset[\"train\"].select([i for i in range(50)])\n",
    "IMDB_dataset_[\"test\"] = IMDB_dataset[\"test\"].select([i for i in range(50)])\n",
    "print(f'Original keys: {IMDB_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Tokenize dataset \"\"\"\n",
    "\n",
    "# Instantiate a tokenizer using the BERT base cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    # Apply truncation to limit the maximum sequence length\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "# Adds output of of tokenizer to the dataset dictionary\n",
    "IMDB_dataset_tokenized = IMDB_dataset_.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename the label column to labels because the model expects the argument to be named labels\n",
    "IMDB_dataset_tokenized = IMDB_dataset_tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(f'Keys after tokenization: {IMDB_dataset_tokenized[\"train\"][0].keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07c7f0-a685-4942-98f9-8659650ad832",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (distilbert-base-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a12c920-7e8f-4ca8-b603-d8f3a9e01a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "num_class_IMDB=len(set(IMDB_dataset_[\"train\"]['label']))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_class_IMDB)\n",
    "                                                            \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79981c8d-e335-4ffb-b01a-3ba4171f9690",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c794b-f8df-4458-bb7c-7e1f3f399ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "\"\"\" Convert the linear layers into LoRA layers \"\"\"\n",
    "\n",
    "# Define lora configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Specify the task type as sequence classification\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate  \n",
    "    target_modules=['q_lin','k_lin','v_lin'] # which modules\n",
    ")\n",
    "\n",
    "# Apply lora congifuration to the model to replace linear layers with LoRA layers\n",
    "model = get_peft_model(model, lora_config)\n",
    "# lora.mark_only_lora_as_trainable(model)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\"\"\" Define evaluation function \"\"\"\n",
    "def evaluate(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "\"\"\" Train the model \"\"\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY=0.01\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Set the batch size for training \n",
    "    per_device_eval_batch_size=BATCH_SIZE, # Set the batch size for validation\n",
    "    weight_decay= WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=IMDB_dataset_tokenized[\"train\"],\n",
    "    eval_dataset=IMDB_dataset_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=evaluate\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1eb93e-29ae-48e4-b4bb-42ec0ce305b9",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec228d-ba09-40bf-94ff-f128c7876424",
   "metadata": {},
   "source": [
    "##### Training loss and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbc707-aeab-4547-b711-058e751ac355",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history_lora = trainer.state.log_history\n",
    "get_metric_lora = lambda metric, log_history_lora: [log[metric] for log in log_history_lora if metric in log]\n",
    "\n",
    "eval_accuracy_lora=get_metric_lora('eval_accuracy',log_history_lora)\n",
    "eval_loss_lora=get_metric_lora('eval_loss',log_history_lora)\n",
    "plt.plot(eval_accuracy_lora,label='eval_accuracy')\n",
    "plt.plot(eval_loss_lora,label='eval_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13e69b-24b5-4893-925a-49a20c1fd32e",
   "metadata": {},
   "source": [
    "## Instruction-based fine-tuning of generative models (w/ SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830352fb-39d0-4fa3-925b-f2a801dbee6f",
   "metadata": {},
   "source": [
    "### Supervised full fine-tuning - OPT model - open assistant dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a54372-1ae8-408c-a2f5-2ed6103fa1fa",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b9ad80-42f3-416d-8020-af58efed840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['text'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "open_assistant_dataset_ = copy.deepcopy(open_assistant_dataset)\n",
    "open_assistant_dataset_[\"train\"] = open_assistant_dataset_[\"train\"].select([i for i in range(500)])\n",
    "open_assistant_dataset_[\"test\"] = open_assistant_dataset_[\"test\"].select([i for i in range(200)])\n",
    "print(f'Original keys: {open_assistant_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Instantiate tokenizer \"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4d707-7baf-4fc8-a59f-e91b4790161a",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (opt-350m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c56ff1a-d7a2-4368-b16d-2edba3994395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9768de-69cb-41a8-beb7-126f20673508",
   "metadata": {},
   "source": [
    "#### DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c4bcb77-8f76-48fc-bdcd-3162289b27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, \n",
    "                                           response_template=response_template, \n",
    "                                           tokenizer=tokenizer, \n",
    "                                           mlm=False\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cd514-7a51-4ed7-bd6c-66c283cf1aa9",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466e152-b71d-4db8-94d0-84f3f2ed9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = SFTConfig(\n",
    "    # output_dir=\"/tmp\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    #gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Instantiate the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=open_assistant_dataset_[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d62f07-f20f-4017-bade-9bd4b8d0603a",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f853a2-9e31-4157-b236-7efcb52f7691",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41191aa-9fe2-4a25-8720-9d8452bbb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83ec91-8691-4d84-aebd-5a2d08286541",
   "metadata": {},
   "source": [
    "##### Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89687bb-eb1d-4e7c-b0e1-dc8e42c7405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,max_new_tokens=70)\n",
    "generated_text = generator(prompt)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3fd51-983b-4717-8768-bcbb23f97e82",
   "metadata": {},
   "source": [
    "### Supervised LoRA fine-tuning - OPT model - CodeAlpaca dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d22446-2ea9-4a7b-87ba-43239cc44530",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96434d0f-edaf-4144-a254-534628851d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['output', 'instruction', 'input'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Select subset of the data \"\"\"\n",
    "code_alpaca_dataset_ = copy.deepcopy(code_alpaca_dataset)\n",
    "print(f'Original keys: {code_alpaca_dataset_[0].keys()}')\n",
    "\n",
    "# Select data that does not have any 'input'\n",
    "code_alpaca_dataset_ = code_alpaca_dataset_.filter(lambda example: example[\"input\"] == '')\n",
    "# Optional: reduce size to speed up training\n",
    "# code_alpaca_dataset_ = code_alpaca_dataset_.select([i for i in range(500)])\n",
    "# Split into train and test sets\n",
    "code_alpaca_dataset_ = code_alpaca_dataset_.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\"\"\" Instantiate tokenizer \"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef6f1d-7e28-4884-a165-7b63bdfb10ba",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (opt-350m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6658e4e7-ce8c-4ac5-a349-139acc0e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf77520-6ec7-46c0-95f9-4ff87d27c9e4",
   "metadata": {},
   "source": [
    "#### DataCollator and pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64e652ed-a587-42ae-8e20-e6e2152534b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Instruction: {example['instruction'][i]}\\n ### Response: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Response:\"\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd9087-ad56-447c-860a-3cd1249801ff",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71b26d-c6a5-4844-ad19-a01e9ada59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_metric\n",
    "\n",
    "\"\"\" Convert the linear layers into LoRA layers \"\"\"\n",
    "\n",
    "# Define lora configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model\n",
    ")\n",
    "\n",
    "# Apply lora congifuration to the model to replace linear layers with LoRA layers\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "LR = 5e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    # learning_rate=LR,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    max_seq_length=5000,\n",
    "    do_eval=True,\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "# Instantiate the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=code_alpaca_dataset_[\"train\"],\n",
    "    eval_dataset=code_alpaca_dataset_[\"test\"],\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a7fd4-4dfb-4e70-9c40-b677b7cdfa0c",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f72ab-a804-4fc5-9c14-74a59cafcfeb",
   "metadata": {},
   "source": [
    "##### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215c075-9b37-4952-9fbc-c8e09a7472f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history_lora = trainer.state.log_history\n",
    "train_loss = [log[\"loss\"] for log in log_history_lora if \"loss\" in log]\n",
    "steps = [log[\"step\"] for log in log_history_lora if \"loss\" in log]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "color = 'tab:red'\n",
    "plt.plot(steps, train_loss, color=color)\n",
    "plt.xlabel('Steps', color=color)\n",
    "plt.ylabel('Training loss', color=color)\n",
    "plt.tick_params(axis='y', color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a310bb-46a5-403e-8240-b6e9efc6bddd",
   "metadata": {},
   "source": [
    "##### Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20108270-664a-4af7-ab4d-8b834f5c32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instruction_ExpectedResponse_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        subset = self.dataset[i]\n",
    "        instruction = []\n",
    "        expected_response = []\n",
    "        for instruction_text, output_text in zip(subset['instruction'], subset['output']):\n",
    "            instruction.append (\n",
    "                f\"### Instruction:\\n{instruction_text}\"\n",
    "                f\"\\n\\n### Response:\\n\"\n",
    "            )\n",
    "\n",
    "            expected_response.append(output_text)\n",
    "        return instruction, expected_response\n",
    "\n",
    "# instructions = formatting_prompts_func_no_response()\n",
    "inference_dataset = Instruction_ExpectedResponse_Dataset(code_alpaca_dataset_[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22fe53-04b9-40b0-9799-06c1d1014d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        device=device, \n",
    "                        batch_size=2, \n",
    "                        max_length=50, \n",
    "                        truncation=True, \n",
    "                        padding=False,\n",
    "                        return_full_text=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Due to resource limitation, only apply the function on 3 records using \"instructions_torch[:10]\"\n",
    "    instructions, expected_response = inference_dataset[:3]\n",
    "    generator_text_iterator= generator(instructions,\n",
    "                                  max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice\n",
    "                                  num_beams=5,\n",
    "                                  early_stopping=True,)\n",
    "\n",
    "predicted_response=[]\n",
    "for i,text in enumerate(generator_text_iterator):\n",
    "    print(instructions[i])\n",
    "    print(text[0]['generated_text'])\n",
    "    predicted_response.append(text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411d3dd-5e24-44a7-aa9c-260374a71703",
   "metadata": {},
   "source": [
    "##### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9661a9-d9e3-4522-a74e-4ad10e8bc72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = sacrebleu.compute(predictions=predicted_response,\n",
    "                                 references=expected_response)\n",
    "print(list(bleu_score.keys()))\n",
    "print(round(bleu_score[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769193d-d1a2-4281-b00a-0e5300b9ff23",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171d812-e229-4b26-8db0-7156790e8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f5902-09d3-4cbe-9613-6c1381b08ef1",
   "metadata": {},
   "source": [
    "## Reward models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244230c-d28f-4d36-a0f7-92a297dd3238",
   "metadata": {},
   "source": [
    "### Supervised LoRA fine-tuning - GPT2 model - Synthetic instruct GPT-J pairwise dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd217f0-a8b6-4083-a15e-beb80e6de304",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "943e4533-66b0-4072-8fc4-0cd7ce843821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original keys: dict_keys(['prompt', 'chosen', 'rejected'])\n",
      "Keys after tokenization: dict_keys(['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\"\"\" Make a copy of the dataset \"\"\"\n",
    "synthetic_pairwise_dataset_ = copy.deepcopy(synthetic_pairwise_dataset)\n",
    "print(f'Original keys: {synthetic_pairwise_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Instantiate tokenizer \"\"\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\"\"\" Pre-process and tokenize dataset \"\"\"\n",
    "\n",
    "def preprocess_and_tokenize_function(example):\n",
    "    # Combine 'prompt' with 'chosen' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
    "    prompt_chosen = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"chosen\"]   \n",
    "    \n",
    "    # Combine 'prompt' with 'rejected' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
    "    prompt_rejected = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"rejected\"]\n",
    "    \n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    prompt_chosen_tokenized = tokenizer(prompt_chosen, truncation=True, max_length = 1024, padding=\"max_length\")\n",
    "    prompt_rejected_tokenized = tokenizer(prompt_rejected, truncation=True, max_length = 1024, padding=\"max_length\")\n",
    "    \n",
    "    # Return the tokenized inputs as a dictionary\n",
    "    return {\n",
    "        \"input_ids_chosen\": prompt_chosen_tokenized[\"input_ids\"],  # Token IDs for 'chosen' responses\n",
    "        \"attention_mask_chosen\": prompt_chosen_tokenized[\"attention_mask\"],  # Attention masks for 'chosen' responses\n",
    "        \"input_ids_rejected\": prompt_rejected_tokenized[\"input_ids\"],  # Token IDs for 'rejected' responses\n",
    "        \"attention_mask_rejected\": prompt_rejected_tokenized[\"attention_mask\"],  # Attention masks for 'rejected' responses\n",
    "    }\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "# Adds output of of tokenizer to the dataset dictionary\n",
    "synthetic_pairwise_dataset_['train'] = synthetic_pairwise_dataset_['train'].map(preprocess_and_tokenize_function, \n",
    "                                                                                remove_columns=['prompt',\"chosen\", \"rejected\"]\n",
    "                                                                               )\n",
    "\n",
    "print(f'Keys after tokenization: {synthetic_pairwise_dataset_[\"train\"][0].keys()}')\n",
    "\n",
    "\"\"\" Remove samples for which either the chosen prompt or rejected prompt is too long and has been truncated \"\"\"\n",
    "\n",
    "def filter_eos(example):\n",
    "    eos_token_id = tokenizer.eos_token_id  \n",
    "\n",
    "    chosen_ids = example['input_ids_chosen']\n",
    "    rejected_ids = example['input_ids_rejected']\n",
    "\n",
    "    # Check if the prompts end with the eos token. If not, they have been truncated and should be removed.\n",
    "    ends_with_eos_chosen = chosen_ids[-1] == eos_token_id if chosen_ids else False\n",
    "    ends_with_eos_rejected = rejected_ids[-1] == eos_token_id if rejected_ids else False\n",
    "\n",
    "    return ends_with_eos_chosen and ends_with_eos_rejected\n",
    "\n",
    "synthetic_pairwise_dataset_ = synthetic_pairwise_dataset_.filter(filter_eos)\n",
    "\n",
    "\"\"\" Split into train and test sets \"\"\"\n",
    "synthetic_pairwise_dataset_ = synthetic_pairwise_dataset_[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3867d4a-c1a8-4ce3-8ba7-4c3ea0c0033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 26502\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 6626\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_pairwise_dataset_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e863297-03a2-4756-9ef2-8a9e00bb59a8",
   "metadata": {},
   "source": [
    "#### Load pre-trained model (gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8f3752e-336a-420b-b8cd-bcb627ff5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=1)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda3c1f-0e7d-49ce-9ebf-30822681bb4b",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5525e9d-3544-4e0f-b321-e062af15fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer, RewardConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "\"\"\" Convert the linear layers into LoRA layers \"\"\"\n",
    "\n",
    "# Define lora configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Task type should be causal language model\n",
    "    inference_mode=False, # Indicates that the configuration is for training mode rather than inference\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],  # Modules to apply LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    ")\n",
    "\n",
    "LR = 1.41e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Reduce batch size\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Instantiate the trainer\n",
    "trainer = RewardTrainer(\n",
    "    model,\n",
    "    train_dataset=synthetic_pairwise_dataset_[\"train\"],\n",
    "    eval_dataset=synthetic_pairwise_dataset_[\"test\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39120886-6d5a-492a-9b27-72ad1efb80e3",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb013c-2021-494e-badb-7c325adab928",
   "metadata": {},
   "source": [
    "##### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d80281-a515-467e-9406-6c100fc101a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "train_loss = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "steps = [log[\"step\"] for log in log_history if \"loss\" in log]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "color = 'tab:red'\n",
    "plt.plot(steps, train_loss, color=color)\n",
    "plt.xlabel('Steps', color=color)\n",
    "plt.ylabel('Training loss', color=color)\n",
    "plt.tick_params(axis='y', color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b75ca-786f-4a16-8ac3-f20890415dfa",
   "metadata": {},
   "source": [
    "##### Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a7952-5aee-4117-9ec4-582e08c80a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643f2fd-5835-4dfe-8807-ff41081f96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(\"./saved_models/reward_model\", num_labels=1).to(device)\n",
    "\n",
    "def get_sample_without_padding(dataset, idx, chosen):\n",
    "    \"\"\"Removes padding from the input_ids and attention_mask tensors.\"\"\"\n",
    "    if chosen:\n",
    "        input_ids = torch.tensor([dataset[idx][\"input_ids_chosen\"]])\n",
    "        attention_mask = torch.tensor([dataset[idx][\"attention_mask_chosen\"]])\n",
    "    else:\n",
    "        input_ids = torch.tensor([dataset[idx][\"input_ids_rejected\"]])\n",
    "        attention_mask = torch.tensor([dataset[idx][\"attention_mask_rejected\"]])\n",
    "        \n",
    "    first_padding_index = (attention_mask == 0).nonzero(as_tuple=True)[1].min().item() if torch.any(attention_mask == 0) else attention_mask.size(1)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids[:, :first_padding_index],\n",
    "        'attention_mask': attention_mask[:, :first_padding_index]\n",
    "    }\n",
    "\n",
    "idx = 10\n",
    "chosen_sample = get_sample_without_padding(synthetic_pairwise_dataset_[\"test\"], idx, chosen = True)\n",
    "rejected_sample = get_sample_without_padding(synthetic_pairwise_dataset_[\"test\"], idx, chosen = False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_chosen = model(**chosen_sample)\n",
    "    outputs_rejected = model(**rejected_sample)\n",
    "logit_chosen = outputs_chosen.logits\n",
    "logit_rejected = outputs_rejected.logits\n",
    "print(\"Chosen score :\",logit_chosen)\n",
    "print(\"Rejected score :\",logit_rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af9729-adea-424d-8506-1657a6daf2cf",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b61f82-ec1f-409d-b678-3ceaf78b294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1605f5-651f-4323-bbdc-8b4144afd3d7",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069cffa-3f19-4af0-aeb0-a915a25de6b0",
   "metadata": {},
   "source": [
    "### Dense Passage Retrieval (DPR) encoder & LLM generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa7a55-37c8-4fbf-ba15-4e43fe5f5002",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b69a9daf-f417-4df7-a725-4e3360c0d3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.',\n",
       " 'We appreciate your cooperation in maintaining a smoke-free and safe environment for all.',\n",
       " '2.\\tRecruitment Policy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "def read_and_split_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # Split the text into paragraphs (simple split by newline characters)\n",
    "    paragraphs = text.split('\\n')\n",
    "    # Filter out any empty paragraphs or undesired entries\n",
    "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "# Read the text file and split it into paragraphs\n",
    "paragraphs = read_and_split_text('data/companyPolicies.txt')\n",
    "random.shuffle(paragraphs)\n",
    "paragraphs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ad431-f1c8-44ff-8ffb-295f150ffa4b",
   "metadata": {},
   "source": [
    "#### Instantiate tokenizers & encoders for the context, questions and LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94894a10-714d-4731-bbff-363615c109b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoderTokenizer, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRQuestionEncoder, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\"\"\" Instantiate context tokenizer and encoder\"\"\"\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "\"\"\" Instantiate question tokenizer and encoder\"\"\"\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "\"\"\" Instantiate LLM tokenizer and encoder\"\"\"\n",
    "LLM_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "LLM_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "LLM_model.generation_config.pad_token_id = LLM_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6a010-00a7-4e0d-967d-c727590799f7",
   "metadata": {},
   "source": [
    "#### Embed the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcd7cd79-e176-4ca5-838c-58448c083b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    # Encode a list of texts into embeddings\n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        # Tokenize the context\n",
    "        context_inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        # Encode the context to get the embedding\n",
    "        context_embedding = context_encoder(**context_inputs)\n",
    "        embeddings.append(context_embedding.pooler_output)\n",
    "    return torch.cat(embeddings).detach().numpy() # Aggregation\n",
    "\n",
    "# encode the context paragraphs to create embeddings.\n",
    "context_embeddings = encode_contexts(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7489f47-220a-4164-a7b4-245b3ccc45f0",
   "metadata": {},
   "source": [
    "#### Embed the question and find relevant context using FAISS\n",
    "\n",
    "FAISS = Facebook AI Similarity Search\n",
    "\n",
    "`IndexFlatL2` is one of the simplest and most used indexes in FAISS. It computes the Euclidean distance (L2 norm) between the query vector and the dataset vectors to determine similarity. This method is straightforward but very effective for many use cases where the exact distance calculation is crucial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c11c292-5986-46c5-aec8-83ad2d6568ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert list of numpy arrays into a single numpy array\n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "embedding_dim = context_embeddings_np.shape[1]  # This should match the dimension of your embeddings\n",
    "\n",
    "# Create a FAISS index for the embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(context_embeddings_np)  # Add the context embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77f57b3f-c6a8-4a1a-b870-80b8f0b46150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5):\n",
    "    \n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba951f3-94e0-4eb2-bbdd-d3f7f41aa720",
   "metadata": {},
   "source": [
    "#### Generate answers using GPT2 model with retrieved information as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db02dfaf-311a-48a7-90b3-af8fdf183549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts, max_len=50, min_len=40, length_penalty=2.0, num_beams=4):\n",
    "    # Concatenate the retrieved contexts to form the input\n",
    "    input_text = question + ' '.join(contexts)\n",
    "    LLM_inputs = LLM_tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate output using GPT2\n",
    "    summary_ids = LLM_model.generate(\n",
    "        LLM_inputs['input_ids'],\n",
    "        max_new_tokens=max_len,\n",
    "        min_length=min_len,\n",
    "        length_penalty=length_penalty,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=LLM_tokenizer.eos_token_id\n",
    "    )\n",
    "    return LLM_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "223a4e4d-e657-412c-ad41-a19b227d1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: what is mobile policy?4.\tMobile Phone Policy The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes. Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations. The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring\n"
     ]
    }
   ],
   "source": [
    "question = \"what is mobile policy?\"\n",
    "\n",
    "_,I =search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "top_contexts = [paragraphs[idx] for idx in I[0]] \n",
    "answer = generate_answer(question, top_contexts)\n",
    "\n",
    "print(\"Generated Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-lab-kernel",
   "language": "python",
   "name": "jupyter-lab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
