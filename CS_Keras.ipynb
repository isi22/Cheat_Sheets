{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a34b703-83bc-481d-94fc-f4fedfb04dbf",
   "metadata": {},
   "source": [
    "# Keras Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6405660-6c05-44fb-b30f-5f9abd23cac8",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6905e-00ec-4bf6-b361-8cffaf460b4e",
   "metadata": {},
   "source": [
    "### Custom Layers (Subclassing)\n",
    "- Allow you to define your own operations in a neural network\n",
    "- Allows implementation of custom functionality not provided by existing Keras layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61637e6b-d39b-4d80-a102-a01e5c578812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CustomLayer(Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959ab87-464f-483f-9a6b-6a066a5d2d5e",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c86eb2a-4298-4820-82ca-3ca3249de871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "input_layer = Input(shape=(28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37cfef-6e5a-4939-8cc2-8fa93d03c0c4",
   "metadata": {},
   "source": [
    "### Dense\n",
    "- Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e06e5dee-b897-4dd4-925e-5be373761860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.initializers import HeNormal\n",
    "\n",
    "dense_layer = Dense(units=100, activation='relu')\n",
    "\n",
    "# To implement He weight initialization (set the initial weights to avoid issues like vanishing or exploding gradients)\n",
    "# He initialization is suitable for layers with relu activation, helping maintain a stable gradient flow during training.\n",
    "dense_layer = Dense(units=100, activation='relu', kernel_initializer=HeNormal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716de8f-5bef-4662-94c0-54f2feb61787",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "- A filter/kernel is moved across the input image to produce a feature map. \n",
    "- Reduces the spatial dimensions of the input, which is useful for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1be70dc3-1b69-41b2-b81f-7ae3598dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "conv_layer = Conv2D(filters=16, kernel_size=(5, 5), strides=(1, 1), activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22a638-5153-4e9e-be04-1f7339d966b1",
   "metadata": {},
   "source": [
    "### Transpose Convolution\n",
    "- Zeroes are inserted between the elements of the input feature map before applying the convolution operation\n",
    "- Performs the inverse convolution operation, effectively up-sampling the input image to a larger higher resolution size\n",
    "**Applications:** image generation (in generative adversarial networks, i.e. GANs), super-resolution, semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddc516cd-5946-423e-ba94-6fb4d905648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose\n",
    "transpose_conv_layer = Conv2DTranspose(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c393a-d5a1-49e4-a37c-22f77bf90747",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "- A regularization technique that helps prevent overfitting in neural networks. \n",
    "- Some of the input units are randomly set to zero at each update cycle. This prevents the model from becoming overly reliant on any specific neurons, which encourages the network to learn more robust features that generalize better to unseen data.\n",
    "- Dropout is only applied during training, not during inference.\n",
    "- The dropout rate is a hyperparameter that determines the fraction of neurons to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cab0318-d0c3-47e7-87ab-950fe64a1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "dropout_layer = Dropout(rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f6402-0b55-4382-8ad7-157de23c3f80",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "- A regularization technique used to improve the training stability and speed of neural networks. \n",
    "- The output of a previous layer is normalized by re-centering (mean of zero) and re-scaling (variance of one) the data, which helps in stabilizing the learning process. By reducing the internal covariate shift (the changes in the distribution of layer inputs), batch normalization allows the model to use higher learning rates, which often speeds up convergence.\n",
    "- It is applied during both training and inference, although its behavior varies slightly between the two phases.\n",
    "- Introduces two learnable parameters that allow the model to scale and - shift the normalized output, which helps in restoring the model's representational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99a48e97-d2db-4996-b067-582dea5a7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "batch_norm_layer = BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192e1ec-ebfe-4566-84ab-b13bc878189b",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11f4e8-71f4-4125-828c-eb7b04f40c01",
   "metadata": {},
   "source": [
    "### Custom Models (Subclassing)\n",
    "- Allows you to define custom and dynamic models\n",
    "- Particularly useful when the forward pass cannot be defined statically\n",
    "- Widely used for custom training loops and non-standard architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ad01d0-2157-468a-9e32-a63e9faeb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "class CustomModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        #Define layers\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559fe3e-7e4e-4a93-84ac-3f8f49c5b0a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conventional Fully Connected Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16bf75-97de-415a-ae62-521dc5b0f090",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "717d7528-3c12-4e8f-b6b3-2a4b5f439b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_44\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_44\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_226 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_227 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_228 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_226 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m1,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_227 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m2,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_228 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,110</span> (16.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,110\u001b[0m (16.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,110</span> (16.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,110\u001b[0m (16.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "def regression_NN(input_shape, output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=output_shape))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_regression_NN = regression_NN(input_shape=20, output_shape = 10)\n",
    "model_regression_NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8116e-1a26-46b1-b18f-17c5ac77d64c",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59ed9d27-4b38-4893-8231-e39aba4dd457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_47\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_47\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_235 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_236 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_237 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_235 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m1,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_236 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m2,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_237 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,110</span> (16.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,110\u001b[0m (16.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,110</span> (16.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,110\u001b[0m (16.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "def classification_NN(input_shape, output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=output_shape, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047dd1f-b14a-4ade-aeb4-8e35814f63ab",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "**Applications:** image recognition, object detection, computer vision\n",
    "\n",
    "**Features:**\n",
    "- Convolutional layers: extract features from the input image\n",
    "- Pooling layers: downsample the feature maps to reduce dimensionality\n",
    "- Fully connected layers: perform final classification\n",
    "\n",
    "**Advanced CNN architectures:** VGG, ResNet, inception networks, deeper networks\n",
    "\n",
    "#### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d279d78-332e-4dce-9ad5-fa7fa87c0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "def simple_CNN(input_shape,output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    model.add(Conv2D(filters=16, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Optional: repeat convolutional and pooling layers\n",
    "    model.add(Conv2D(filters=8, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dense(units=output_shape, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fb10f-f9c9-4a50-86cd-29c060e827bc",
   "metadata": {},
   "source": [
    "#### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0172082a-f132-494d-84c9-9bf9a5480f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "def VGG_CNN(input_shape,output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dense(units=output_shape, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef430b8-291c-4869-a82e-f8cc9e1b18ce",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83dc037-3099-4c2c-a9b0-74c0ac86da88",
   "metadata": {},
   "source": [
    "- Introduces residual connections, which help train deep networks by addressing the vanishing gradient problem. \n",
    "- Residual connections allow the network to learn identity mappings, making it easier to train deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6abfd3-9328-47e5-8f41-233547c4c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Add, Activation\n",
    "\n",
    "def ResNet_CNN(input_shape,output_shape):\n",
    "    \n",
    "    def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "        shortcut = x\n",
    "        x = Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([x, shortcut])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=64, kernel_size=(7,7), strides=2, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = Flatten()(x)\n",
    "    output_layer = Dense(output_shape, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdcd57-acd9-4463-927c-8cf77f9891d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pre-trained: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f06e1943-b476-4a96-b08d-95fe974928f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def VGG16_CNN(input_shape, output_shape):\n",
    "    \n",
    "    # Load the VGG16 model pre-trained on ImageNet\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=256, activation='relu'))\n",
    "    model.add(Dense(units=output_shape, activation='sigmoid'))\n",
    "    \n",
    "    #Optional: fine-tuning \n",
    "    for layer in base_model.layers[-4:]: #Unfreeze the last four layers of the VGG16 model\n",
    "        layer.trainable = True \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b1f5a-11f4-4cbc-bdf1-e8d2ca16cb67",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)\n",
    "- Take in the output from previous data points\n",
    "- Very good at modeling patterns and sequences of data\n",
    "\n",
    "**Applications:** image generation, handwriting generation, auto-captioning, genomes, stock markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e7fc-415f-43d7-9ec5-ce1a0ff4f770",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "413150c6-d27a-4105-9390-339cbfc1a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "\n",
    "def simple_RNN(input_shape,output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    model.add(SimpleRNN(units=50, activation='relu'))\n",
    "    model.add(Dense(units=output_shape))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc32d0-f0ac-4ba3-97cd-6cba2751906d",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "beaa5f0a-c283-4269-9e56-4abe725d0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "\n",
    "def LSTM_RNN(input_shape,output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    model.add(LSTM(units=50, activation='relu'))\n",
    "    model.add(Dense(units=output_shape))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edba46-09d2-4b35-8052-0eda6c63ac24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformers\n",
    "- Leverage self attention mechanisms to process input data in parallel\n",
    "- Consists of two main parts, the encoder and the decoder with:\n",
    "  - Self attention mechanism layers: captures dependencies that are far apart in the input sequence\n",
    "  - Feed forward neural network layers: transforms input data\n",
    "- Like RNNs and LSTMs, good for sequential data (e.g. natural language text and time series data), with the additional advantage of being better at handling longer sequencies and parallerisation\n",
    "- The key components of the transformer model include an embedding layer, multiple transformer blocks, and a final dense layer for output prediction.\n",
    "\n",
    "**Examples:** BERT, GPT, image transformers\n",
    "\n",
    "**Applications:** Natural Language Processing (NLP) (e.g. machine translation, question answering, text summarisation, text-to-image), time-series forecasting, computer vision, speech recognition, reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e252e9fb-242f-4d60-af22-924266babb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Dense, LayerNormalization, Dropout, Input, Flatten, MultiHeadAttention, Embedding\n",
    "from keras.models import Sequential, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "#Define the Multi-Head Self-Attention mechanism\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "#implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): #computes the attention scores and weighted sum of the values \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): #splits the input into multiple heads for parallel attention computation\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): #applies the self-attention mechanism and combines the heads\n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "#Define the Transformer block\n",
    "class TransformerBlock(Layer): \n",
    "#combines multi-head self-attention with a feed-forward neural network and normalization layers\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        \n",
    "        #EITHER:\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        #OR:\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        self.ffn = Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) #dropout used to prevent overfitting\n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): #applies the self-attention, followed by the feedforward network with residual connections and layer normalization\n",
    "        # EITHER:\n",
    "        # attn_output = self.att(inputs) \n",
    "        # OR:\n",
    "        attn_output = self.att(inputs, inputs)    \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de2ee7-1e2b-48b7-8d5d-8803c8621310",
   "metadata": {},
   "source": [
    "#### Prediction: sequential data tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "101bb904-b342-48ce-908d-d239c64c1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Transformer Encoder\n",
    "class TransformerEncoder(Layer):\n",
    "#composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture.\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x  \n",
    "\n",
    "def Predictions_Transformer(input_shape, output_shape):\n",
    "#defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output\n",
    "\n",
    "    # Hyperparameters\n",
    "    embed_dim = 128 \n",
    "    num_heads = 8 \n",
    "    ff_dim = 512 \n",
    "    num_layers = 4 \n",
    "\n",
    "    # Define the Transformer Encoder \n",
    "    transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "    input_layer = Input(shape=input_shape) \n",
    "\n",
    "    # Project the inputs to the embed_dim \n",
    "    x = Dense(units=embed_dim)(input_layer) #embedding layer\n",
    "    encoder_outputs = transformer_encoder(x) \n",
    "    flatten = Flatten()(encoder_outputs) \n",
    "    output_layer = Dense(units=output_shape)(flatten) \n",
    "    model = Model(input_layer, output_layer) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6332a8-7c44-4b1b-939c-c4109f2e9cc5",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a32e62bc-1557-4ab2-9a36-887c231e31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "class TextGen_TransformerModel(Model):  # Model is now properly imported\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TextGen_TransformerModel, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def positional_encoding(self, seq_length, embed_dim):\n",
    "        angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, embed_dim):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        x = self.embedding(inputs)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)  # Pass training argument correctly\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "    \n",
    "def TextGen_Transformer():\n",
    "\n",
    "    # Hyperparameters \n",
    "    embed_dim = 256 \n",
    "    num_heads = 4 \n",
    "    ff_dim = 512 \n",
    "    num_layers = 4 \n",
    "\n",
    "    # Build the Transformer model \n",
    "    model = TextGen_TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9aa11-97d1-4f7e-874a-e1377e86e42e",
   "metadata": {},
   "source": [
    "### Q-learning (Reinforcement Learning)\n",
    "- Reinforcement learning algorithm\n",
    "  - **Agents** choose from a set of **Actions**\n",
    "  - **Actions** impact the **Environment**, which impacts **Agents** via **Rewards**\n",
    "  - **Rewards** are unknown and must be estimated by the **Agent**\n",
    "  - The process repeats dynamically, so that **Agents** learn how to estimate **Rewards**\n",
    "- The essence of Q-learning lies in the Q-value function, Q(s, a): yhe Q-values are updated iteratively using the Bellman equation, which incorporates both the immediate reward and the estimated future rewards.\n",
    "- The steps to implement Q-learning with Keras include initializing the environments, building the Q-network, training the Q-network, and evaluating the agent.\n",
    "\n",
    "**Applications**: recommendation engines, marketing, automated bidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e650d2ab-e87b-4c1f-95ae-7097dc41bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # a toolkit for developing and comparing reinforcement learning algorithms (OpenAI Gym library)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input \n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the environment \n",
    "# CartPole-v1 is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over \n",
    "# (a common benchmark for reinforcement learning algorithms)\n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# Global settings\n",
    "episodes = 10  # Number of episodes\n",
    "batch_size = 32  # Size of the mini-batch for training\n",
    "epsilon = 1.0  # Starting with a high exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.99  # Faster decay rate for epsilon after each episode\n",
    "memory_size=2000\n",
    "\n",
    "memory = deque(maxlen=memory_size)  # Memory buffer to store experiences\n",
    "\n",
    "# Define state size and action size based on the environment\n",
    "state_size = env.observation_space.shape[0]  # State space size from the environment\n",
    "action_size = env.action_space.n  # Number of possible actions from the environment\n",
    "\n",
    "# Define the model building function (takes the state as input and outputs Q-values for each action)\n",
    "def build_model(state_size, action_size): \n",
    "    model = Sequential() \n",
    "    model.add(Input(shape=(state_size,)))  # Use Input layer to specify the input shape \n",
    "    model.add(Dense(24, activation='relu')) \n",
    "    model.add(Dense(24, activation='relu')) \n",
    "    model.add(Dense(action_size, activation='linear')) #linear activation function for the output layer, as we are predicting continuous Q-values\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001)) \n",
    "    return model \n",
    "\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Replay memory\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    \"\"\"Store experience in memory.\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def replay(batch_size):  # Increased batch size\n",
    "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Skip replay if there's not enough experience\n",
    "\n",
    "    minibatch = random.sample(memory, batch_size)  # Sample a random batch from memory\n",
    "    \n",
    "    # Extract information for batch processing\n",
    "    states = np.vstack([x[0] for x in minibatch])\n",
    "    actions = np.array([x[1] for x in minibatch])\n",
    "    rewards = np.array([x[2] for x in minibatch])\n",
    "    next_states = np.vstack([x[3] for x in minibatch])\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "    \n",
    "    # Predict Q-values for the next states in batch\n",
    "    q_next = model.predict(next_states)\n",
    "    # Predict Q-values for the current states in batch\n",
    "    q_target = model.predict(states)\n",
    "    \n",
    "    # Vectorized update of target values\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += 0.95 * np.amax(q_next[i])  # Update Q value with the discounted future reward\n",
    "        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n",
    "    \n",
    "    # Train the model with the updated targets in batch\n",
    "    model.fit(states, q_target, epochs=1, verbose=0)  # Train in batch mode\n",
    "\n",
    "    # Reduce exploration rate (epsilon) after each training step\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "def act(state):\n",
    "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose a random action\n",
    "    act_values = model.predict(state)  # Exploit: predict action based on the state\n",
    "    return np.argmax(act_values[0])  # Return the action with the highest Q-value\n",
    "\n",
    "# Define the number of episodes you want to train the model for\n",
    "episodes = 10  # You can set this to any number you prefer\n",
    "train_frequency = 5  # Train the model every 5 steps\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Unpack the tuple returned by env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200):  # Limit to 200 time steps per episode\n",
    "        action = act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done)  # Store experience\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "        \n",
    "        # Train the model every 'train_frequency' steps\n",
    "        if time % train_frequency == 0:\n",
    "            replay(batch_size)  # Call replay with larger batch size for efficiency\n",
    "\n",
    "env.close()\n",
    "\n",
    "# evaluate the performance of the trained Q-Learning agent\n",
    "for e in range(10):  \n",
    "\n",
    "    state, _ = env.reset()  # Unpack the state from the tuple \n",
    "    state = np.reshape(state, [1, state_size])  # Reshape the state correctly \n",
    "    for time in range(500):  \n",
    "        env.render()  \n",
    "        action = np.argmax(model.predict(state)[0])  \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # Unpack the five return values \n",
    "        done = terminated or truncated  # Check if the episode is done \n",
    "        next_state = np.reshape(next_state, [1, state_size])  \n",
    "        state = next_state  \n",
    "        if done:  \n",
    "            print(f\"episode: {e+1}/10, score: {time}\")  \n",
    "            break  \n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125161e-b135-418b-8273-930700008321",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "- Unsupervised learner\n",
    "- A type of neural network used to learn efficient representations of data for the purpose of dimensionality reduction or feature learning\n",
    "- Consist of three main parts:\n",
    "  - Encoder: this part compresses the input into a smaller latent space representation\n",
    "  - Bottleneck: compressed representation\n",
    "  - Decoder: this part reconstructs the input from the latent space representation. \n",
    "- The key idea is that the autoencoder is trained to minimize the difference between the input and the reconstructed output, forcing the network to learn meaningful representations of the data.\n",
    "\n",
    "**Applications:** data denoising, dimensionality reduction, and feature learning\n",
    "\n",
    "**Advanced Autoencoder architectures:** \n",
    "- Variational Autoencoders (VAEs): have probabilistic:  elements used for generating new data samples\n",
    "- Convolutional Autoencoder: use convolutional layers and are effective for image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1e00433-060c-4bde-a0d4-66e41b01b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "\n",
    "def Autoencoder(input_shape,output_shape):\n",
    "    \n",
    "    input_layer = Input(input_shape)\n",
    "    \n",
    "    # Encoder \n",
    "    encoded = Dense(64, activation='relu')(input_layer) \n",
    "\n",
    "    # Bottleneck \n",
    "    bottleneck = Dense(32, activation='relu')(encoded) \n",
    "\n",
    "    # Decoder \n",
    "    decoded = Dense(64, activation='relu')(bottleneck) \n",
    "    output_layer = Dense(784, activation='sigmoid')(decoded) \n",
    "\n",
    "    # Autoencoder model \n",
    "    model = Model(input_layer, output_layer) \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d3796-b48e-49d7-9830-177bc01b8993",
   "metadata": {},
   "source": [
    "### Diffusion models\n",
    "\n",
    "- Unsupervised learner\n",
    "- Probabilistic models that generate data by iteratively refining a noisy initial sample\n",
    "- They start with a random noise and gradually apply a series of transformations to produce a coherent data sample\n",
    "- Simulate the physical process of diffusion, where particles spread out from regions of high concentration to regions of low concentration\n",
    "- Diffusion models work by defining a forward process and a reverse process\n",
    "- Consist of three main parts:\n",
    "  - Encoder: this part compresses the input into a smaller latent space representation\n",
    "  - Bottleneck: compressed representation\n",
    "  - Decoder: this part reconstructs the input from the latent space representation. \n",
    "\n",
    "**Applications**: image generation, image enhancement/denoising, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "82c2c30e-d162-447f-874c-0e2511360d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "\n",
    "def DiffusionModel(input_shape,output_shape):\n",
    "    \n",
    "    input_layer = Input(shape=(28, 28, 1))\n",
    "    \n",
    "    #Encoder\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)  # Reduced filters\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)  # Reduced size\n",
    "    \n",
    "    #Encoder\n",
    "    x = Dense(28*28*32, activation='relu')(x)  # Reduced size\n",
    "    x = Reshape((28, 28, 32))(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "    x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "    output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65c79b-e64c-4020-b5eb-2deab813334a",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks (GANs)\n",
    "- Unsupervised learner\n",
    "- Consist of two networks:\n",
    "  - Generator network: generates new data instances that resemble the training data.\n",
    "  - Discriminator network: evaluates the authenticity of the generated data.\n",
    "- The two networks are trained simultaneously through adversarial training: the generator tries to fool the discriminator while the discriminator tries to distinguish between real and fake data. This adversarial process leads to the generator producing increasingly realistic data. \n",
    "\n",
    "**Applications:** image generation, text-to-image synthesis, image-to-image translation, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "99cc3d74-f87a-47e5-a3cb-a81e88dd5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/isabelle/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_61\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_61\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_264 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_265 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_266 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_267 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">803,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_264 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m25,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_265 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_266 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_267 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │       \u001b[38;5;34m803,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,520</span> (5.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,493,520\u001b[0m (5.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,489,936</span> (5.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,489,936\u001b[0m (5.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_62\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_62\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_268 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_269 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_270 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_28 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_268 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_269 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_270 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">533,505</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m533,505\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">533,505</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m533,505\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_184\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_184\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">533,505</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_71 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_61 (\u001b[38;5;33mSequential\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │     \u001b[38;5;34m1,493,520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_62 (\u001b[38;5;33mSequential\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m533,505\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,027,025</span> (7.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,027,025\u001b[0m (7.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,489,936</span> (5.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,489,936\u001b[0m (5.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,089</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m537,089\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
    "\n",
    "# Define the generator model\n",
    "# The generator takes a random noise vector as an input and generates a synthetic image\n",
    "def build_generator(): \n",
    "    model = Sequential() \n",
    "    model.add(Dense(256, input_dim=100)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(Dense(512)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(Dense(1024)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(Dense(28 * 28 * 1, activation='tanh')) \n",
    "    model.add(Reshape((28, 28, 1))) \n",
    "    return model \n",
    "\n",
    "# Build the generator \n",
    "generator = build_generator() \n",
    "generator.summary()\n",
    "\n",
    "# Define the discriminator model\n",
    "# The discriminator takes an image as an input and outputs a probability indicating whether the image is real or fake\n",
    "def build_discriminator(): \n",
    "    model = Sequential() \n",
    "    model.add(Flatten(input_shape=(28, 28, 1))) \n",
    "    model.add(Dense(512)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(Dense(256)) \n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    return model \n",
    "\n",
    "# Build and compile the discriminator \n",
    "discriminator = build_discriminator() \n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "discriminator.summary()\n",
    "\n",
    "# Create the GAN by stacking the generator and the discriminator\n",
    "# The GAN takes a noise vector as an input, generates a synthetic image using the generator, and classifies the image using the discriminator\n",
    "def build_gan(generator, discriminator): \n",
    "    discriminator.trainable = False # The discriminator is set to non-trainable when compiling the GAN to ensure that only the generator is updated during the adversarial training. \n",
    "    gan_input = Input(shape=(100,)) # Create an input layer for the noise vector\n",
    "    generated_image = generator(gan_input) # Pass the noise vector through the generator to produce a synthetic image. \n",
    "    gan_output = discriminator(generated_image) #Pass the synthetic image through the discriminator to get the classification.\n",
    "    model = Model(gan_input, gan_output) # Compile the GAN using binary cross-entropy loss and the Adam optimizer.\n",
    "\n",
    "    return model \n",
    "\n",
    "# Build the GAN \n",
    "gan = build_gan(generator, discriminator) \n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95e6ef90-26d6-42d6-9b1d-b76deeafe4c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Training loop \u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Train the discriminator \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     idx = np.random.randint(\u001b[32m0\u001b[39m, \u001b[43mx_train\u001b[49m.shape[\u001b[32m0\u001b[39m], batch_size) \n\u001b[32m     15\u001b[39m     real_images = x_train[idx] \n\u001b[32m     16\u001b[39m     noise = np.random.normal(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, (batch_size, \u001b[32m100\u001b[39m)) \n",
      "\u001b[31mNameError\u001b[39m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Training parameters \n",
    "\n",
    "batch_size = 64 \n",
    "epochs = 50\n",
    "sample_interval = 10\n",
    "\n",
    "# Adversarial ground truths \n",
    "real = np.ones((batch_size, 1)) \n",
    "fake = np.zeros((batch_size, 1)) \n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs): \n",
    "    # Train the discriminator \n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size) \n",
    "    real_images = x_train[idx] \n",
    "    noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "    generated_images = generator.predict(noise) \n",
    "    d_loss_real = discriminator.train_on_batch(real_images, real) \n",
    "    d_loss_fake = discriminator.train_on_batch(generated_images, fake) \n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
    "\n",
    "    # Train the generator \n",
    "    noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "    g_loss = gan.train_on_batch(noise, real) \n",
    "\n",
    "    # Print the progress \n",
    "    if epoch % sample_interval == 0: \n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}] [D accuracy: {100 * d_loss[1]}%] [G loss: {g_loss}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27f85f-6433-4c4d-a141-e0b72ec0d9b3",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d42823-422f-4376-8dad-4618ba979494",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "612d0e9a-4b20-4092-a39c-01f5d1acd2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Custom augmentation function\n",
    "def add_random_noise(image):\n",
    "    noise = np.random.normal(0, 0.1, image.shape)\n",
    "    return image + noise\n",
    "\n",
    "# Create an instance of ImageDataGenerator with basic augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    \n",
    "    #basic augmentations\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    \n",
    "    #normalizations\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    \n",
    "    #custom augmentations\n",
    "    preprocessing_function=add_random_noise\n",
    ")\n",
    "\n",
    "#Only required if featurewise_center or featurewise_std_normalization or zca_whitening are set to True\n",
    "# datagen.fit(train_data) #Computes the internal data stats related to the data-dependent transformations\n",
    "\n",
    "# Visualizing multiple augmented versions of the same image\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, batch in enumerate(datagen.flow(x, batch_size=1)):\n",
    "#     if i >= 4:  # Show only 4 versions\n",
    "#         break\n",
    "#     plt.subplot(2, 2, i+1)\n",
    "#     plt.imshow(batch[0].astype('uint8'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b3209-ad17-4e86-87cc-c08802aa5de2",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5002081-df3f-4f55-9bc7-28facded83af",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "40ff648e-3cf7-49a8-8d5d-03f44b5d769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_gen__transformer_model_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"text_gen__transformer_model_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_34            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_35            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_36            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_37            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_263 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_34            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_35            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_36            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_37            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_263 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "# input_shape = X_train.shape[1:]\n",
    "# output_shape = y_train.shape[1] # number of categories\n",
    "# model = simple_CNN(input_shape, output_shape)\n",
    "\n",
    "model = TextGen_Transformer()\n",
    "\n",
    "model.summary()\n",
    "# plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0018c-316c-4d7e-ad8d-94afd43e48fa",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f148f1-bbd2-4d35-a5cd-8bb82181b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer options\n",
    "optimizer='adam'\n",
    "\n",
    "#Loss options\n",
    "loss='mean_squared_error' # Regression\n",
    "loss='categorical_crossentropy' # Multi-classification\n",
    "loss='binary_crossentropy' # Binary classification\n",
    "\n",
    "#Metrics options\n",
    "metrics=['accuracy']\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer, loss, metrics) # Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69acc-9cd8-4934-bb13-485e627e87c1",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75124d45-0034-46f6-9b7e-98c0aec36369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.3, epochs=n_epochs, verbose=2) #with validation set, but training data not split into training and validation\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, verbose=2) #with validation set, training data has already been split\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs = n_epochs, verbose=2) #when using data generators\n",
    "model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size) #without validation set\n",
    "model.fit(X_train_noisy, X_train, epochs=n_epochs, batch_size=batch_size, shuffle=True, validation_data=(X_test, X_test)) #autoencoders, diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d04675-07a9-4cf3-9e5c-3011bcc17705",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d125d7-694a-4159-9585-fff4b83725f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores = model.evaluate(test_generator, verbose=0)\n",
    "print(\"Loss: {} \\n Accuracy: {} \\n Error: {}\".format(scores[0], scores[1], 100-scores[1]*100))\n",
    "\n",
    "#Visualise training results (plot the loss and accuracy curves)\n",
    "train_history = model.history.history  # After training\n",
    "\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['loss'])\n",
    "plt.plot(train_history['val_loss'])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813faf5d-d160-41ae-99ca-cce8e26f37af",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c243e-9f98-4a03-8ed4-3186c16a1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model-building function \n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a RandomSearch Tuner \n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space \n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run the hyperparameter search \n",
    "tuner.search(X_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    "# Display a summary of the results \n",
    "tuner.results_summary() \n",
    "\n",
    "# Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    "#Build and Train the Model with Best Hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "# Evaluate the model on the test set \n",
    "test_loss, test_acc = model.evaluate(X_val, y_val) \n",
    "print(f'Test accuracy: {test_acc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54914c-8916-4cb0-9dab-3c6c8571a675",
   "metadata": {},
   "source": [
    "### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a657adb9-b54c-47a2-b695-a890d1b35cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8ebbd-482a-4045-be4e-de9e3b58132a",
   "metadata": {},
   "source": [
    "### Custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f56014-2566-4116-ae53-bbb5d364f45b",
   "metadata": {},
   "source": [
    "Can be implemented instead of using the standard Keras fit method for a more tailored training process.\n",
    "\n",
    "**Advantages**:\n",
    "- Custom loss functions and optimization strategies.\n",
    "- Enable advanced logging and monitoring\n",
    "- Flexibility for research\n",
    "- Integration with custom operations and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def6ab9-519e-4a51-b8e0-5f7901985df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = keras.optimizers.Adam()\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e1e33-e633-429f-8728-446cd6a5b7ec",
   "metadata": {},
   "source": [
    "### Custom callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cb24f-8fef-49ff-a656-bf6d706ed7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback \n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n",
    "        \n",
    "model.fit(X_train, y_train, epochs=10, callbacks=[CustomCallback()]) #with validation set, but training data not split into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44935403-2ac5-4c95-a423-d147059398ff",
   "metadata": {},
   "source": [
    "## Save and load Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4c9bf-5933-4a5f-b032-2fe9f3a26ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('filename.h5')\n",
    "pretrained_model = keras.models.load_model('filename.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a9171-9f0e-4340-b2b4-0972e4072bca",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba03331-31cd-4f42-8936-00a1916e5b60",
   "metadata": {},
   "source": [
    "### Mixed precision training\n",
    "Mixed precision training involves using both 16-bit and 32-bit floating-point types to speed up training on modern GPUs, leading to faster computation and reduced memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "07cbf66d-64cc-4883-8461-94aa5e1dd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e4209-3b65-4f6b-90e4-a1e679cd2a39",
   "metadata": {},
   "source": [
    "### Model pruning\n",
    "Model pruning reduces the number of parameters in a model by removing less significant connections or neurons, making it more efficient without a substantial loss in accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "47297017-a0b7-47fd-a3a1-4a317cb0a60c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip -q install --upgrade tensorflow-model-optimization\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfmot\u001b[39;00m\n\u001b[32m      4\u001b[39m prune_low_magnitude =tfmot.sparsity.keras.prune_low_magnitude\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Apply pruning to model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/__init__.py:86\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# To ensure users only access the expected public API, the API structure is\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# created in the `api` directory. Import all api modules.\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Import API modules for Tensorflow Model Optimization.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Module containing code for clustering.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_scope\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_weights\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m strip_clustering\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_wrapper\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clusterable_layer\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_registry\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     28\u001b[39m attrgetter = operator.attrgetter  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clusterable_layer\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_algorithm\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_model_optimization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     24\u001b[39m layers = keras.layers\n\u001b[32m     25\u001b[39m ClusteringAlgorithm = clustering_algorithm.ClusteringAlgorithm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/keras/compat.py:41\u001b[39m\n\u001b[32m     37\u001b[39m     keras_internal = tf.keras\n\u001b[32m     38\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m keras_internal\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m keras = \u001b[43m_get_keras_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massign\u001b[39m(ref, value, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     44\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf, \u001b[33m'\u001b[39m\u001b[33massign\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow_model_optimization/python/core/keras/compat.py:35\u001b[39m, in \u001b[36m_get_keras_instance\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m version_fn = \u001b[38;5;28mgetattr\u001b[39m(tf.keras, \u001b[33m'\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version_fn \u001b[38;5;129;01mand\u001b[39;00m version_fn().startswith(\u001b[33m'\u001b[39m\u001b[33m3.\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m   \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras_internal\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,unused-import\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m   keras_internal = tf.keras\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'"
     ]
    }
   ],
   "source": [
    "# !pip -q install tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude =tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Apply pruning to model\n",
    "pruning_params = {'prunint_schedule':\n",
    "                  tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
    "                                                       final_sparsity=0.5,\n",
    "                                                       begin_step=0,\n",
    "                                                       end_step=2000)}\n",
    "\n",
    "model_pruned=prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc8c70-bd30-4e56-8278-529608477c5e",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Reduces the precision of the numbers used to represent the models' weights, which helps in deploying models on edge devices by reducing memory usage and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "94d34b01-7dfd-41cb-b2cb-7b53159ac65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/tmp25e5_r9b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/tmp25e5_r9b/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1743091259.210109 11935859 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1743091259.210332 11935859 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2025-03-27 16:00:59.212552: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/tmp25e5_r9b\n",
      "2025-03-27 16:00:59.212726: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-03-27 16:00:59.212732: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/tmp25e5_r9b\n",
      "2025-03-27 16:00:59.214358: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2025-03-27 16:00:59.214656: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2025-03-27 16:00:59.221279: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/tmp25e5_r9b\n",
      "2025-03-27 16:00:59.222464: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 10095 microseconds.\n",
      "2025-03-27 16:00:59.239188: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "Could not translate MLIR to FlatBuffer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConverterError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m converter = tf.lite.TFLiteConverter.from_keras_model(model)\n\u001b[32m      3\u001b[39m converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m quantized_model = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1175\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1174\u001b[39m   model_object = flatbuffer_utils.convert_bytearray_to_object(model)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _check_model_use_buffer_offset(model_object):\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m   1177\u001b[39m   model = _deduplicate_readonly_buffers(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1129\u001b[39m, in \u001b[36m_convert_and_export_metrics\u001b[39m\u001b[34m(self, convert_func, *args, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m @convert_phase(Component.OPTIMIZE_TFLITE_MODEL)\n\u001b[32m   1124\u001b[39m def _optimize_tflite_model(\n\u001b[32m   1125\u001b[39m     self, model, quant_mode, debug_options, quant_io=True\n\u001b[32m   1126\u001b[39m ):\n\u001b[32m   1127\u001b[39m   \"\"\"Apply optimizations on a TFLite model.\"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m   # Disable TFLite quantization pass when\n\u001b[32m   1130\u001b[39m   # `experimental_use_stablehlo_quantizer` is set to `True`. StableHLO\n\u001b[32m   1131\u001b[39m   # Quantizer performs quantization during the conversion step, which happens\n\u001b[32m   1132\u001b[39m   # before `_optimize_tflite_model`.\n\u001b[32m   1133\u001b[39m   if (\n\u001b[32m   1134\u001b[39m       quant_mode.is_integer_quantization()\n\u001b[32m   1135\u001b[39m       and not self.experimental_use_stablehlo_quantizer\n\u001b[32m   1136\u001b[39m   ):\n\u001b[32m   1137\u001b[39m     in_type, out_type = self.inference_input_type, self.inference_output_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1636\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_keras_3():\n\u001b[32m-> \u001b[39m\u001b[32m1636\u001b[39m   \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[32m   1638\u001b[39m   \u001b[38;5;66;03m# Keras 3 model `export` by default saves model.__call__ with\u001b[39;00m\n\u001b[32m   1639\u001b[39m   \u001b[38;5;66;03m# training=True. Need to export the model call with training=False for\u001b[39;00m\n\u001b[32m   1640\u001b[39m   \u001b[38;5;66;03m# inference only and TFLite conversion.\u001b[39;00m\n\u001b[32m   1641\u001b[39m   export_archive = keras.export.ExportArchive()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1617\u001b[39m, in \u001b[36m_convert_as_saved_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1608\u001b[39m \u001b[38;5;129m@convert_phase\u001b[39m(\n\u001b[32m   1609\u001b[39m     Component.PREPARE_TF_MODEL, SubComponent.CONVERT_KERAS_TO_SAVED_MODEL\n\u001b[32m   1610\u001b[39m )\n\u001b[32m   1611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_keras_to_saved_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_dir):\n\u001b[32m   1612\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Save Keras model to the SavedModel format.\u001b[39;00m\n\u001b[32m   1613\u001b[39m \n\u001b[32m   1614\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    output_dir: The output directory to save the SavedModel.\u001b[39;00m\n\u001b[32m   1616\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1617\u001b[39m \u001b[33;03m  Returns:\u001b[39;00m\n\u001b[32m   1618\u001b[39m \u001b[33;03m    graph_def: The frozen GraphDef.\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[33;03m    input_tensors: List of input tensors.\u001b[39;00m\n\u001b[32m   1620\u001b[39m \u001b[33;03m    output_tensors: List of output tensors.\u001b[39;00m\n\u001b[32m   1621\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m   1622\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_keras_3\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/lite.py:1407\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(self, graph_def, input_tensors, output_tensors)\u001b[39m\n\u001b[32m   1398\u001b[39m \u001b[38;5;66;03m# Skip running grappler when there are no optimizers to run. If not,\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[38;5;66;03m# grappler will run with the default optimizer set and it will lead to\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[38;5;66;03m# causing an unexpected behavior.\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grappler_config.graph_options.rewrite_options.optimizers:\n\u001b[32m   1402\u001b[39m   graph_def = _run_graph_optimizations(\n\u001b[32m   1403\u001b[39m       graph_def,\n\u001b[32m   1404\u001b[39m       input_tensors,\n\u001b[32m   1405\u001b[39m       output_tensors,\n\u001b[32m   1406\u001b[39m       config=grappler_config,\n\u001b[32m-> \u001b[39m\u001b[32m1407\u001b[39m       graph=frozen_func.graph,\n\u001b[32m   1408\u001b[39m   )\n\u001b[32m   1409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m graph_def\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:212\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     report_error_message(\u001b[38;5;28mstr\u001b[39m(converter_error))\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m converter_error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Re-throws the exception.\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    214\u001b[39m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    204\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m converter_error.errors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:995\u001b[39m, in \u001b[36mconvert_graphdef\u001b[39m\u001b[34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m subgraph.operators:\n\u001b[32m    994\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m op.inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    996\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m i, input_tensor_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(op.inputs):\n\u001b[32m    997\u001b[39m     \u001b[38;5;66;03m# Ignore mutable tensors.\u001b[39;00m\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op.mutatingVariableInputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    999\u001b[39m       \u001b[38;5;66;03m# Ignore invalid tensors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:367\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_model_flags\u001b[39m(\n\u001b[32m    354\u001b[39m     change_concat_input_ranges=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    355\u001b[39m     allow_nonexistent_arrays=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    360\u001b[39m     **_,\n\u001b[32m    361\u001b[39m ):\n\u001b[32m    362\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Builds the model flags object from params.\u001b[39;00m\n\u001b[32m    363\u001b[39m \n\u001b[32m    364\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    change_concat_input_ranges: Boolean to change behavior of min/max ranges for\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m      inputs and outputs of the concat operator for quantized models. Changes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[33;03m      the ranges of concat operator overlap when true. (default False)\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[33;03m    allow_nonexistent_arrays: Allow specifying array names that don't exist or\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[33;03m      are unused in the final graph. (default False)\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[33;03m    saved_model_dir: Filepath of the saved model to be converted. This value\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m      will be non-empty only when the saved model import path will be used.\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m      Otherwises, the graph def-based conversion will be processed.\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    saved_model_version: SavedModel file format version of The saved model file\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m      to be converted. This value will be set only when the SavedModel import\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03m      path will be used.\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[33;03m    saved_model_tags: Set of string saved model tags, formatted in the\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m      comma-separated value. This value will be set only when the SavedModel\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m      import path will be used.\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03m    saved_model_exported_names: Names to be exported (default: export all) when\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[33;03m      the saved model import path is on. This value will be set only when the\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m      SavedModel import path will be used.\u001b[39;00m\n\u001b[32m    382\u001b[39m \n\u001b[32m    383\u001b[39m \u001b[33;03m  Returns:\u001b[39;00m\n\u001b[32m    384\u001b[39m \u001b[33;03m    model_flags: protocol buffer describing the model.\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m   model_flags = _model_flags_pb2.ModelFlags()\n\u001b[32m    387\u001b[39m   model_flags.change_concat_input_ranges = change_concat_input_ranges\n",
      "\u001b[31mConverterError\u001b[39m: Could not translate MLIR to FlatBuffer."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model = converter.convert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-lab-kernel",
   "language": "python",
   "name": "jupyter-lab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
