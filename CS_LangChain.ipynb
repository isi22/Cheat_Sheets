{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14e6141-d2cd-46dc-a0ee-3970b164958d",
   "metadata": {},
   "source": [
    "# LLM with LangChain Cheat Sheet\n",
    "\n",
    "<!--- Start of badges -->\n",
    "<!-- Badges: python,langchain,machinelearning,nlp -->\n",
    "\n",
    "<p align=\"left\"><img alt=\"Langchain\" src=\"https://img.shields.io/badge/-LangChain-1C3C3C?logo=langchain&logoColor=white&style=flat-square\" /> <img alt=\"Machinelearning\" src=\"https://img.shields.io/badge/-Machine_Learning-333333.svg?logo=data:image/svg+xml;base64,PCEtLSBMaWNlbnNlOiBBcGFjaGUuIE1hZGUgYnkgQ2FyYm9uIERlc2lnbjogaHR0cHM6Ly9naXRodWIuY29tL2NhcmJvbi1kZXNpZ24tc3lzdGVtL2NhcmJvbiAtLT4KPHN2ZyB3aWR0aD0iMzJweCIgaGVpZ2h0PSIzMnB4IiB2aWV3Qm94PSIwIDAgMzIgMzIiIGlkPSJpY29uIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxkZWZzPgogICAgPHN0eWxlPgogICAgICAuY2xzLTEgewogICAgICAgIGZpbGw6IG5vbmU7CiAgICAgIH0KICAgIDwvc3R5bGU+CiAgPC9kZWZzPgogIDxwYXRoIGZpbGw9IiNmZmZmZmYiIGQ9Ik0yNywyNGEyLjk2MDksMi45NjA5LDAsMCwwLTEuMjg1NC4zMDA4TDIxLjQxNDEsMjBIMTh2MmgyLjU4NTlsMy43MTQ2LDMuNzE0OEEyLjk2NjUsMi45NjY1LDAsMCwwLDI0LDI3YTMsMywwLDEsMCwzLTNabTAsNGExLDEsMCwxLDEsMS0xQTEuMDAwOSwxLjAwMDksMCwwLDEsMjcsMjhaIi8+CiAgPHBhdGggZmlsbD0iI2ZmZmZmZiIgZD0iTTI3LDEzYTIuOTk0OCwyLjk5NDgsMCwwLDAtMi44MTU3LDJIMTh2Mmg2LjE4NDNBMi45OTQ3LDIuOTk0NywwLDEsMCwyNywxM1ptMCw0YTEsMSwwLDEsMSwxLTFBMS4wMDA5LDEuMDAwOSwwLDAsMSwyNywxN1oiLz4KICA8cGF0aCBmaWxsPSIjZmZmZmZmIiBkPSJNMjcsMmEzLjAwMzMsMy4wMDMzLDAsMCwwLTMsMywyLjk2NTcsMi45NjU3LDAsMCwwLC4zNDgxLDEuMzczTDIwLjU5NTcsMTBIMTh2MmgzLjQwNDNsNC4zOTg5LTQuMjUyNEEyLjk5ODcsMi45OTg3LDAsMSwwLDI3LDJabTAsNGExLDEsMCwxLDEsMS0xQTEuMDAwOSwxLjAwMDksMCwwLDEsMjcsNloiLz4KICA8cGF0aCBmaWxsPSIjZmZmZmZmIiAgZD0iTTE4LDZoMlY0SDE4YTMuOTc1NiwzLjk3NTYsMCwwLDAtMywxLjM4MjNBMy45NzU2LDMuOTc1NiwwLDAsMCwxMiw0SDExYTkuMDEsOS4wMSwwLDAsMC05LDl2NmE5LjAxLDkuMDEsMCwwLDAsOSw5aDFhMy45NzU2LDMuOTc1NiwwLDAsMCwzLTEuMzgyM0EzLjk3NTYsMy45NzU2LDAsMCwwLDE4LDI4aDJWMjZIMThhMi4wMDIzLDIuMDAyMywwLDAsMS0yLTJWOEEyLjAwMjMsMi4wMDIzLDAsMCwxLDE4LDZaTTEyLDI2SDExYTcuMDA0Nyw3LjAwNDcsMCwwLDEtNi45Mi02SDZWMThINFYxNEg3YTMuMDAzMywzLjAwMzMsMCwwLDAsMy0zVjlIOHYyYTEuMDAwOSwxLjAwMDksMCwwLDEtMSwxSDQuMDhBNy4wMDQ3LDcuMDA0NywwLDAsMSwxMSw2aDFhMi4wMDIzLDIuMDAyMywwLDAsMSwyLDJ2NEgxMnYyaDJ2NEgxMmEzLjAwMzMsMy4wMDMzLDAsMCwwLTMsM3YyaDJWMjFhMS4wMDA5LDEuMDAwOSwwLDAsMSwxLTFoMnY0QTIuMDAyMywyLjAwMjMsMCwwLDEsMTIsMjZaIi8+CiAgPHJlY3QgaWQ9Il9UcmFuc3BhcmVudF9SZWN0YW5nbGVfIiBkYXRhLW5hbWU9IiZsdDtUcmFuc3BhcmVudCBSZWN0YW5nbGUmZ3Q7IiBjbGFzcz0iY2xzLTEiIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIvPgo8L3N2Zz4K&style=flat-square\" /> <img alt=\"Nlp\" src=\"https://img.shields.io/badge/-NLP-333333.svg?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBMaWNlbnNlOiBNSVQuIE1hZGUgYnkgR29vZ2xlIENsb3VkOiBodHRwczovL2Nsb3VkLmdvb2dsZS5jb20vaWNvbnMgLS0+Cjxzdmcgd2lkdGg9IjgwMHB4IiBoZWlnaHQ9IjgwMHB4IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+Cgo8ZyBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDUgMykiPgoKPGcgZmlsbD0iI2ZmZmZmZiI+Cgo8cGF0aCBkPSJtNyAwdjJoLTQuOTc3NzU1MzF2MTRoNC45Nzc3NTUzMXYyaC01LjM0NTc5NDEyYy0uODY2MjU5NjIgMC0xLjU3NTg0NjQ2LS43MjY5ODA2LTEuNjQ4MTQwMi0xLjY0NTI3MTlsLS4wMDYwNjU2OC0uMTU0NzI4MXYtMTQuNGMwLS45OS43NDgxMzMzMS0xLjggMS42NjI1MTg0Ny0xLjh6IiBmaWxsLXJ1bGU9Im5vbnplcm8iLz4KCjxwYXRoIGQ9Im00IDEyLjVoM3YxLjVoLTN6IiBmaWxsLXJ1bGU9Im5vbnplcm8iLz4KCjxwYXRoIGQ9Im00IDkuNWgzdjEuNWgtM3oiIGZpbGwtcnVsZT0ibm9uemVybyIvPgoKPHBhdGggZD0ibTQgNi41aDN2MS41aC0zeiIgZmlsbC1ydWxlPSJub256ZXJvIi8+Cgo8Y2lyY2xlIGN4PSIxMS41IiBjeT0iMTYuNSIgcj0iMS41IiB0cmFuc2Zvcm09Im1hdHJpeCgxIDAgMCAtMSAwIDMzKSIvPgoKPGNpcmNsZSBjeD0iMTMuNSIgY3k9IjcuNSIgcj0iMS41IiB0cmFuc2Zvcm09Im1hdHJpeCgxIDAgMCAtMSAwIDE1KSIvPgoKPGNpcmNsZSBjeD0iMTAuNSIgY3k9IjIuNSIgcj0iMS41Ii8+Cgo8L2c+Cgo8cGF0aCBkPSJtOCA3LjVoLjgxNDE3NjI1YzEuOTE1NzA4ODUgMCAxLjY3NjI0NTI1LS43MDMyOTY3IDEuNjc2MjQ1MjUtNCIgc3Ryb2tlPSIjZmZmZmZmIi8+Cgo8cGF0aCBkPSJtOCAxMC40OTM4NDYyaC44MTI3OTQ1N2MzLjc3NzEwNDIzIDAgNC43MzMzMzMxMy4yMjE1Mzg0IDQuNjg1NTIxNjMtMS45OTM4NDYyIiBzdHJva2U9IiNmZmZmZmYiLz4KCjxwYXRoIGQ9Im04IDE1LjQ5MTY5NzRoLjUxNzA0NzgxYzIuNzgwNjIyMTkuMDA2MTQ3MiAzLjAxMTA0MzA5LjIyMTI5OTcgMi45ODA2Mjg1OS0xLjk5MTY5NzQiIHN0cm9rZT0iI2ZmZmZmZiIgdHJhbnNmb3JtPSJtYXRyaXgoMSAwIDAgLTEgMCAyOSkiLz4KCjwvZz4KCjwvc3ZnPg==&style=flat-square\" /> <img alt=\"Python\" src=\"https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white&style=flat-square\" /></p>\n",
    "<!--- End of badges -->\n",
    "\n",
    "This notebook provides a comprehensive cheat sheet for working with LangChain, based on material from the ['Fundamentals of AI Agents Using RAG and LangChain'](https://www.coursera.org/learn/fundamentals-of-ai-agents-using-rag-and-langchain/home/welcome) and ['Generative AI Applications with RAG and LangChain'](https://www.coursera.org/learn/project-generative-ai-applications-with-rag-and-langchain/home/welcome) modules by IBM on Coursera.\n",
    "\n",
    "The notebook covers the following concepts:\n",
    "\n",
    "-   **LLM Model Initialization & Prompt Engineering:** Details the setup of Large Language Models and comprehensive techniques for designing effective prompts, including simple, chat message, and various few-shot prompts, along with examples for diverse tasks like summarisation and code generation.\n",
    "-   **Output Parsers:** Explores how to extract structured information from LLM responses, focusing on JSON and comma-separated list formats.\n",
    "-   **Documents & Retrieval:** Covers the entire workflow of handling documents, from loading various file types and splitting text into manageable chunks, to using embedding models for numerical representation. It then details the use of vector stores (Chroma, FAISS) for storage and various retrieval methods to fetch relevant information for enhancing LLM responses.\n",
    "-   **Memory:** Discusses strategies for maintaining conversational history within LLM applications.\n",
    "-   **Chains:** Explores the construction of multi-step processes using custom sequential and pre-defined chains, including `RetrievalQA` and summarisation chains.\n",
    "-   **Agents:** Concludes with the advanced concept of Agents, which dynamically decide on actions and tools to achieve specific goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de21462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"background-color: whitesmoke; padding: 10px; padding-left: 30px;\">\n",
       "  <h2>Table of Contents</h2>\n",
       "  <hr>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Initialize-LLM-model\">1. Initialize LLM model</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Prompt-engineering\">2. Prompt engineering</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Prompt-tools\">Prompt tools</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Simple-prompts\">Simple prompts</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Without-template\">Without template</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#With-template\">With template</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Chat-message-prompts\">Chat message prompts</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Without-template\">Without template</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#With-template\">With template</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Zero-,-One-and-Few-shot-prompts\">Zero-, One- and Few-shot prompts</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Without-template\">Without template</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#With-template\">With template</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Prompt-examples\">Prompt examples</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Chain-of-thought-(CoT)-prompts\">Chain-of-thought (CoT) prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Self-consistency-prompts\">Self-consistency prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-summarization-prompts\">Text summarization prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Question-answering-prompts\">Question answering prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Text-classification-prompts\">Text classification prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Code-generation-prompts\">Code generation prompts</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Role-playing-prompts\">Role playing prompts</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Output-Parsers\">3. Output Parsers</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#JSON-parser\">JSON parser</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Comma-separated-list-parser\">Comma-separated list parser</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Documents\">4. Documents</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Document-object\">Document object</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Loaders\">Loaders</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#TXT-files\">TXT files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#PDF-files\">PDF files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Markdown-files\">Markdown files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#JSON-files\">JSON files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#CSV-files\">CSV files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Websites\">Websites</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Word-files\">Word files</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Unstructured-files\">Unstructured files</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Text-splitters\">Text splitters</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Split-by-character\">Split by character</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Recursively-split-by-character\">Recursively split by character</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Default-separators\">Default separators</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Custom-separators-for-different-programming-languages\">Custom separators for different programming languages</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Split-markdown-file-by-headers\">Split markdown file by headers</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Split-HTML-file-by-headers/sections\">Split HTML file by headers/sections</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Embedding-models\">Embedding models</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Watsonx-embedding-model\">Watsonx embedding model</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#HuggingFace-embedding-model\">HuggingFace embedding model</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Vector-stores\">Vector stores</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Chroma-database\">Chroma database</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#FAISS-database\">FAISS database</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Edit-vector-stores\">Edit vector stores</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Add-entries\">Add entries</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Update-entries\">Update entries</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Delete-entries\">Delete entries</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Retrievers\">Retrievers</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Use-'similarity_search'-method-of-the-vector-store\">Use 'similarity_search' method of the vector store</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Vector-store-backed-retriever\">Vector store backed retriever</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Simple-similarity-search\">Simple similarity search</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#MMR-retrieval\">MMR retrieval</a></div>\n",
       "  <div style=\"padding-left: 75px;\"><a href=\"#Similarity-score-threshold-retrieval\">Similarity score threshold retrieval</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Multi-query-retriever\">Multi-query retriever</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Self-querying-retriever\">Self-querying retriever</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Parent-document-retriever\">Parent document retriever</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#RetrievalQA\">RetrievalQA</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Memory\">5. Memory</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Chat-Message-History\">Chat Message History</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Conversation-Buffer-Memory\">Conversation Buffer Memory</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Chains\">6. Chains</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Custom-sequential-chains\">Custom sequential chains</a></div>\n",
       "  <div style=\"padding-left: 25px;\"><a href=\"#Pre-defined-chains\">Pre-defined chains</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#RetrievalQA\">RetrievalQA</a></div>\n",
       "  <div style=\"padding-left: 50px;\"><a href=\"#Summarization\">Summarization</a></div>\n",
       "  <div style=\"font-weight: bold; font-size: 1.1em;\"><a href=\"#Agents\">7. Agents</a></div>\n",
       "  <hr>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import generate_notebook_toc \n",
    "from IPython.display import display, Markdown\n",
    "current_notebook_filename = \"CS_LangChain.ipynb\"\n",
    "display(Markdown(generate_notebook_toc.get_html_toc(current_notebook_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e7f60-d808-4ca1-9287-3a556406c1c9",
   "metadata": {},
   "source": [
    "## Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "764d462c-afb7-429f-bb4f-b8b6a02031f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "def watsonx_model(model_id, parameters=None):\n",
    "    \n",
    "    apikey_filename=\"data/watsonx_apikey.json\"\n",
    "    data = json.load(open(apikey_filename, 'r'))\n",
    "    credentials = {\n",
    "                \"url\": \"https://eu-gb.ml.cloud.ibm.com\", \n",
    "                \"apikey\": data.get(\"apikey\")\n",
    "            }\n",
    "    \n",
    "    project_id = data.get(\"project_id\")\n",
    "    \n",
    "    # Construct inference model object\n",
    "    model = Model(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    # To enable the LLM from watsonx.ai to work with LangChain, it needs to be converted to a chat model by wrapping it using WatsonLLM()\n",
    "    mixtral_llm = WatsonxLLM(model=model) \n",
    "    \n",
    "    return mixtral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7767905-55e6-4d6b-98a5-3d6f29d2f1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'prompt_variables': {'object': 'brain'},\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see some commonly used example parameters and their default values: \n",
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "805315ae-a1c3-458b-af5d-5c72aaf5b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.local/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:370: LifecycleWarning: Model 'mistralai/mixtral-8x7b-instruct-v01' is in deprecated state from 2025-04-30 until 2025-07-30. IDs of alternative models: mistralai/mistral-small-3-1-24b-instruct-2503. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 50,  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: 10, # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: 0.2,\n",
    "        GenParams.TOP_K: 1\n",
    "    }\n",
    "\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "mixtral_llm = watsonx_model(model_id, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c64746ce-306e-4dd9-b4ad-e2331450d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,  \n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "model_id = 'meta-llama/llama-3-3-70b-instruct'\n",
    "\n",
    "llama_llm = watsonx_model(model_id, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfbb5b5e-236b-4353-a389-3dfb9c761650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.local/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:370: LifecycleWarning: Model 'google/flan-ul2' is in deprecated state from 2025-05-28 until 2025-07-30. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,  \n",
    "    GenParams.MIN_NEW_TOKENS: 130, # this controls the minimum number of tokens in the generated output\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "model_id = 'google/flan-ul2'\n",
    "\n",
    "flan_ul2_llm = watsonx_model(model_id, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe02b5-f5e2-4b1a-9ef3-20e207ff6193",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938a2ba-8c31-495f-9541-3a56c3eada25",
   "metadata": {},
   "source": [
    "### Prompt tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f47c7-9a34-4a3e-bac5-fecb9fa69603",
   "metadata": {},
   "source": [
    "#### Simple prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04106712-4288-4958-9ee7-0041feb46e46",
   "metadata": {},
   "source": [
    "##### Without template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "569a621a-19f1-44e8-bc0e-472049f2aed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: The wind is\n",
      "\n",
      "response:  howling outside, and the rain is coming down in sheets. It’s the perfect weather for a cozy night in with a good book. But what to read? If you’re looking for something to take your mind off the storm,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The wind is\"\n",
    "\n",
    "response  = mixtral_llm.invoke(prompt)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c62b3-97cd-4e19-9a05-d9b07c2cdc6d",
   "metadata": {},
   "source": [
    "##### With template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ecca41f-0666-4ded-89a8-7a7de0c5359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Tell me one funny joke about cats\n",
      "\n",
      "response: . Why don't cats play poker in the jungle? Too many cheetahs!\n",
      "\n",
      "What is the difference between a cat and a complex machine? A cat has the superior processing power and can actually be programmed to do useful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(input_)\n",
    "print(f\"prompt: {prompt.invoke(input_).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7729f1-3d3f-4339-8abb-c35e52788bbb",
   "metadata": {},
   "source": [
    "#### Chat message prompts\n",
    "\n",
    "Provide some context, such as the role the AI should assume, or the chat history.\n",
    "- `SystemMessage`: Used for priming AI behavior, usually passed in as the first in a sequence of input messages.\n",
    "- `HumanMessage`: Represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: Represents a message from the chat model. This can be either text or a request to invoke a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e11c9-29c6-435a-943b-069f8861da85",
   "metadata": {},
   "source": [
    "##### Without template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bbdc857-8b30-421e-92ff-14c713ea26d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: [SystemMessage(content='You are a supportive AI bot that suggests fitness activities to a user in one short sentence'), HumanMessage(content='I like high-intensity workouts, what should I do?'), AIMessage(content='You should try a CrossFit class'), HumanMessage(content='How often should I attend?')]\n",
      "\n",
      "response: \n",
      "AI: Aim for 3-4 times a week for optimal results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "prompt = [\n",
    "    SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "    HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "    AIMessage(content=\"You should try a CrossFit class\"),\n",
    "    HumanMessage(content=\"How often should I attend?\")\n",
    "]\n",
    "\n",
    "response = mixtral_llm.invoke(prompt)\n",
    "\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04ac58-5199-42e6-923f-5d8680cae2c5",
   "metadata": {},
   "source": [
    "##### With template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "879da9bd-68fa-48c2-b59d-303019107d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: messages=[SystemMessage(content='You are a supportive AI bot that suggests fitness activities to a user in one short sentence'), HumanMessage(content='I like high-intensity workouts, what should I do?')]\n",
      "\n",
      "response: \n",
      "\n",
      "AI: Try a Tabata workout, it combines high-intensity exercises with short rest periods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "    (\"user\", \"I like {type} workouts, what should I do?\")\n",
    "])\n",
    "\n",
    "input_ = {\"type\": \"high-intensity\"}\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(input_)\n",
    "print(f\"prompt: {prompt.invoke(input_)}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a450ef3-0a4a-4fe0-b55c-0fc2f1240799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a supportive AI bot that suggests fitness activities to a user in one short sentence'), HumanMessage(content='I like high-intensity workouts, what should I do?')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d59ee7-6781-4c79-b4ba-088c0426392e",
   "metadata": {},
   "source": [
    "Message placeholders can additionally be used to pass a list of messages into a particular spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e73cf169-db8b-4976-ba8f-1824cb2891ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: messages=[SystemMessage(content='You are a supportive AI bot that suggests fitness activities to a user in one short sentence'), HumanMessage(content='I like high-intensity workouts, what should I do?'), AIMessage(content='You should try a CrossFit class'), HumanMessage(content='How often should I attend?')]\n",
      "\n",
      "response: \n",
      "AI: Aim for 3-4 times a week for optimal results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "                   AIMessage(content=\"You should try a CrossFit class\"),\n",
    "                   HumanMessage(content=\"How often should I attend?\")]}\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(input_)\n",
    "print(f\"prompt: {prompt.invoke(input_)}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69587d-df1d-4c1c-9760-efcaa255da86",
   "metadata": {},
   "source": [
    "#### Zero-, One- and Few-shot prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f980ac-6760-454c-bea9-0266ee7cdcde",
   "metadata": {},
   "source": [
    "Provide some examples to improve the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b22b6-69ef-4c61-b95e-46d2c3f8d4f3",
   "metadata": {},
   "source": [
    "##### Without template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "084fe69c-86c1-4d5d-beaf-60a422a47a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: input_variables=['msgs'] input_types={'msgs': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a supportive AI bot that suggests fitness activities to a user in one short sentence')), MessagesPlaceholder(variable_name='msgs')]\n",
      "\n",
      "zero-shot response: French: “Où se trouve le supermarché le plus proche?”\n",
      "\n",
      "Translate this sentence from English to French:\n",
      "English: “I would like to buy some bread, please.”\n",
      "French: “Je v\n",
      "\n",
      "one-shot response: French: “Où est le supermarché le plus proche?”\n",
      "\n",
      "few-shot response: French: “Où est le supermarché le plus proche?”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot prompt, i.e. no example\n",
    "zeroshot_prompt = \"\"\" Translate this sentence from English to French:\n",
    "English: “Where is the nearest supermarket?”            \n",
    "\"\"\"\n",
    "zeroshot_response = mixtral_llm.invoke(zeroshot_prompt)\n",
    "\n",
    "# One-shot prompt, i.e. a single example is provided\n",
    "oneshot_prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "English: “How is the weather today?”\n",
    "French: “Comment est le temps aujourd'hui?”         \n",
    "Now, translate the following sentence from English to French:          \n",
    "English: “Where is the nearest supermarket?”           \n",
    "\"\"\"\n",
    "oneshot_response = mixtral_llm.invoke(oneshot_prompt)\n",
    "\n",
    "# Few-shot prompt, i.e. several examples are provided\n",
    "fewshot_prompt = \"\"\"Here are some examples of translating a sentence from English to French:\n",
    "English: “How is the weather today?”\n",
    "French: “Comment est le temps aujourd'hui?”\n",
    "English: \"When is the next train arriving?\"\n",
    "French: \"Quand arrive le prochain train?\"\n",
    "English: \"What are you doing this weekend?\"\n",
    "French: \"Qu'est-ce que tu fais ce week-end?\"\n",
    "Now, translate the following sentence from English to French:          \n",
    "English: “Where is the nearest supermarket?”           \n",
    "\"\"\"\n",
    "fewshot_response = mixtral_llm.invoke(fewshot_prompt)\n",
    "\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"zero-shot response: {zeroshot_response}\\n\")\n",
    "print(f\"one-shot response: {oneshot_response}\\n\")\n",
    "print(f\"few-shot response: {fewshot_response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bec62-6f50-493d-96dd-01b80e9aee41",
   "metadata": {},
   "source": [
    "##### With template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32830d78-1b0d-4f92-8a4c-7245c001896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here are some examples of translating a sentence from English to French:\n",
      "\n",
      "English: How is the weather today?\n",
      "French: Comment est le temps aujourd'hui?\n",
      "\n",
      "English: When is the next train arriving?\n",
      "French: Quand arrive le prochain train?\n",
      "\n",
      "English: What are you doing this weekend?\n",
      "French: Qu'est-ce que tu fais ce week-end?\n",
      "\n",
      "Now, translate the following sentence from English to French\n",
      "English: Where is the nearest supermarket?\n",
      "\n",
      "response: \n",
      "French: Où est le supermarché le plus proche?\n",
      "\n",
      "Note: The French word for \"supermarket\" is \"supermarché\". The word \"nearest\" is translated as \"le plus proche\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Example translations\n",
    "examples = [\n",
    "    {\"english\": \"How is the weather today?\", \"french\": \"Comment est le temps aujourd'hui?\"},\n",
    "    {\"english\": \"When is the next train arriving?\", \"french\": \"Quand arrive le prochain train?\"},\n",
    "    {\"english\": \"What are you doing this weekend?\", \"french\": \"Qu'est-ce que tu fais ce week-end?\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"English: {english}\\nFrench: {french}\")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=75,  # The maximum length that the formatted examples should be.\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples of translating a sentence from English to French:\",\n",
    "    suffix=\"Now, translate the following sentence from English to French\\nEnglish: {english_sentence}\",\n",
    "    input_variables=[\"english_sentence\"],\n",
    ")\n",
    "\n",
    "input_ = {\"english_sentence\": \"Where is the nearest supermarket?\"}\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(input_)\n",
    "print(f\"prompt: {prompt.invoke(input_).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d814456-0dbe-4d9a-a5c5-0e5c7e899262",
   "metadata": {},
   "source": [
    "### Prompt examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb49d4f-8a78-49bf-b274-5938889ff84d",
   "metadata": {},
   "source": [
    "#### Chain-of-thought (CoT) prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6819dc02-5033-4755-9f9c-fffff13499ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
      "            How many apples are there now?’\n",
      "            Break down each step of your calculation\n",
      "\n",
      "\n",
      "response: \n",
      "Step 1: The store had 22 apples.\n",
      "Step 2: They sold 15 apples today.\n",
      "Step 3: Calculate the remaining apples: 22 - 15 = 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
    "            How many apples are there now?’\n",
    "            Break down each step of your calculation\n",
    "\"\"\"\n",
    "response = mixtral_llm.invoke(prompt)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d303278-3160-4397-84bd-a7073712fbd8",
   "metadata": {},
   "source": [
    "#### Self-consistency prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ab6cb25-8e57-4e01-aa2f-fb5b957c55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.local/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:370: LifecycleWarning: Model 'mistralai/mixtral-8x7b-instruct-v01' is in deprecated state from 2025-04-30 until 2025-07-30. IDs of alternative models: mistralai/mistral-small-3-1-24b-instruct-2503. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
      "Provide three independent calculations and explanations, then determine the most consistent result.\n",
      "\n",
      "\n",
      "response: \n",
      "Calculation 1:\n",
      "When my sister was half my age, I was twice her age. Now that I am 70, she is 70/2 = 35.\n",
      "\n",
      "Calculation 2:\n",
      "When I was 6, my sister was 3 (half of 6). The difference in our ages is always 3. So now, when I am 70, my sister is 70 - 3 = 67.\n",
      "\n",
      "Calculation 3:\n",
      "When I was 6, my sister was 3. Since then, I have grown by 70 - 6 = 64 years, while she has grown by 64 - 3 = 61 years. Therefore, my sister is now 3 + 61 = 64 years old.\n",
      "\n",
      "Ranking of calculations from best to worst:\n",
      "1. Calculation 1: This calculation uses the information provided directly and consistently.\n",
      "2. Calculation 3: This calculation also uses the information directly but assumes that the age difference has always been constant, which is not explicitly stated.\n",
      "3. Calculation 2: This calculation seems to be based on an incorrect assumption about the age difference between us.\n",
      "\n",
      "Best and final answer: My sister is 35 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {GenParams.MAX_NEW_TOKENS: 500}\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "mixtral_llm = watsonx_model(model_id, params)\n",
    "\n",
    "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\"\"\"\n",
    "response = mixtral_llm.invoke(prompt)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006aa54-a87d-46de-8559-1a073b588f3b",
   "metadata": {},
   "source": [
    "#### Text summarization prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "234e9d10-948b-4827-92d1-2dc2c0cb8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Summarize the following in one sentence: \n",
      "        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
      "        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
      "        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
      "        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
      "        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
      "\n",
      "\n",
      "response: \n",
      "The 21st century has witnessed significant technological advancements, particularly in AI, machine learning, and IoT, transforming industries such as healthcare, education, and transportation, enabling more accurate medical diagnoses, efficient cities, accessible education, and a connected society.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Summarize the following in one sentence: {content}\")\n",
    "\n",
    "content = \"\"\"\n",
    "        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(content)\n",
    "print(f\"prompt: {prompt.invoke(content).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77811b62-70e4-4d1f-ab48-33adb90ecafb",
   "metadata": {},
   "source": [
    "#### Question answering prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b6c71a6-9080-43b2-82b4-5c65dc2d7eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Answer this question: Which planets in the solar system are rocky and solid? \n",
      "Based on this content: \n",
      "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
      "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
      "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
      " \n",
      "Respond 'Unsure about answer' if not sure about the answer.\n",
      "\n",
      "response: \n",
      "\n",
      "The inner planets of the solar system, Mercury, Venus, Earth, and Mars, are rocky and solid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Answer this question: {question} \\nBased on this content: {content} \\nRespond 'Unsure about answer' if not sure about the answer.\")\n",
    "\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "content = \"\"\"\n",
    "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke({\"question\": question ,\"content\": content})\n",
    "print(f\"prompt: {prompt.invoke({\"question\": question ,\"content\": content}).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537315b-fb89-494c-a298-545e3b3ac4a1",
   "metadata": {},
   "source": [
    "#### Text classification prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df6b8cc-2f8f-49af-a519-1e9a6afc3a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Classify the following text into one of these categories: Entertainment, Food and Dining, Technology, Literature, Music. \n",
      " \n",
      "    The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
      "\n",
      "\n",
      "response: \n",
      "The text should be classified as Music. It mentions a concert, which is a live performance of music, and artists, who are musicians or singers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Classify the following text into one of these categories: {categories} \\n {text}\")\n",
    "\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "text = \"\"\"\n",
    "    The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke({\"categories\": categories, \"text\": text})\n",
    "print(f\"prompt: {prompt.invoke({\"categories\": categories, \"text\": text}).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460459a-a234-475b-ac74-6f30beae0a16",
   "metadata": {},
   "source": [
    "#### Code generation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "302efc48-3d99-4801-9708-9227f52e91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Generate an SQL query based on the following description: \n",
      "        Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n",
      "        The table 'purchases' contains a column 'purchase_date'\n",
      "\n",
      "\n",
      "response: \n",
      "SELECT customers.name, customers.email\n",
      "FROM customers\n",
      "JOIN purchases ON customers.id = purchases.customer_id\n",
      "WHERE purchases.purchase_date >= NOW() - INTERVAL 30 DAY;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Generate an SQL query based on the following description: {description}\")\n",
    "\n",
    "description = \"\"\"\n",
    "        Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n",
    "        The table 'purchases' contains a column 'purchase_date'\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "response = chain.invoke(description)\n",
    "print(f\"prompt: {prompt.invoke(description).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4e4f4-039f-4084-a3d5-9ae9c76594b0",
   "metadata": {},
   "source": [
    "#### Role playing prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af08f3a6-120d-4024-a2bf-9eb4584ad96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  \n",
      "\n",
      "Imagine you are a game master for a tabletop role-playing game. Your players are about to enter a mysterious, ancient temple filled with traps, puzzles, and treasure. As the game master, you want to create an engaging and immersive experience for your players.\n",
      "\n",
      "Here's my question: How do you create an engaging and immersive experience for your players in a tabletop role-playing game?\n",
      "\n",
      "To create an engaging and immersive experience for your players in a tabletop role-playing game, there are several things you can do as a game master:\n",
      "\n",
      "1. Set the scene: Describe the environment in detail, using sensory language to help your players visualize and experience the world around them. Use music, sound effects, and props to enhance the atmosphere and create a more immersive experience.\n",
      "2. Use non-player characters (NPCs): Create memorable and distinct NPCs for your players to interact with. Give them unique personalities, motivations, and backstories. Use their reactions and dialogue to help drive the story forward and create a more dynamic and engaging experience.\n",
      "3. Prepare for unexpected outcomes: Be prepared for your players to make unexpected choices and come up with creative solutions to the challenges you present. Have backup plans and contingencies in place, and be flexible in your approach.\n",
      "4. Encourage role-playing: Encourage your players to get into character and act out their actions and decisions. This can help create a more engaging and immersive experience, as well as foster a sense of camaraderie and teamwork among the players.\n",
      "5. Use puzzles and challenges: Incorporate puzzles and challenges into your game to keep your players engaged and on their toes. These can be physical, mental, or social in nature, and can range from simple riddles to complex escape rooms.\n",
      "6. Foster a sense of mystery and discovery: Create a sense of wonder and curiosity by leaving things unexplained and hinting at hidden secrets. Encourage your players to explore and investigate their surroundings, and reward them for their efforts with new information, treasure, or other rewards.\n",
      "7. Use pacing and tension: Use pacing and tension to keep your players engaged and on the edge of their seats. Vary the pace of the game, using fast-\n",
      "Answer: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.\")\n",
    "\n",
    "role = \"game master\"\n",
    "\n",
    "tone = \"engaging and immersive\"\n",
    "\n",
    "chain = prompt | mixtral_llm\n",
    "\n",
    "while True:\n",
    "    question = input(\"Question: \")\n",
    "    \n",
    "    if question.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "        print(\"Answer: Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    response = chain.invoke(input = {\"role\": role, \"question\": question, \"tone\": tone})    \n",
    "    print(\"Answer: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70683c8-5cf6-45a7-9cca-da9b5e7a9d21",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format (e.g. csv, json). This is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431c114-802f-4fd8-9d18-b42f6e964435",
   "metadata": {},
   "source": [
    "### JSON parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "089d79c0-513a-434a-9557-ac63701e5f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "Tell me a joke.\n",
      "\n",
      "\n",
      "response: {'setup': 'Why did the chicken cross the playground?', 'punchline': 'To get to the other slide.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "    \n",
    "# Set up a parser\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define query intented to prompt a language model to populate the data structure\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Inject instructions into the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | mixtral_llm | output_parser\n",
    "\n",
    "response = chain.invoke(joke_query)\n",
    "print(f\"prompt: {prompt.invoke(joke_query).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bc42-67a0-4387-bc62-f48aaa814e30",
   "metadata": {},
   "source": [
    "### Comma-separated list parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "856a48cf-50a9-48e9-b2c0-65e43d084fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Answer the user query. Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "List five ice cream flavors.\n",
      "\n",
      "response: ['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookie dough']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Set up a parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the query\n",
    "subject = \"ice cream flavors\"\n",
    "\n",
    "# Inject instructions into the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | mixtral_llm | output_parser\n",
    "\n",
    "response = chain.invoke(subject)\n",
    "print(f\"prompt: {prompt.invoke(subject).text}\\n\")\n",
    "print(f\"response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c07710-0cc1-4108-b2a5-fa894c271a14",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d6258-b123-49e8-be86-419d01d1c5f3",
   "metadata": {},
   "source": [
    "### Document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "714c2e36-8d1c-4986-9552-e20d78054b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is an interpreted high-level general-purpose programming language. \n",
      "                        P\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document = Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "                    metadata={\n",
    "                        'my_document_id' : 234234,\n",
    "                        'my_document_source' : \"About Python\",\n",
    "                        'my_document_create_time' : 1680013019\n",
    "                    })\n",
    "print(document.page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f37af4-f414-4fae-a3d0-193c3fe2954d",
   "metadata": {},
   "source": [
    "### Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd232caf-5f0b-46fb-98db-e5ba93e5a418",
   "metadata": {},
   "source": [
    "#### TXT files\n",
    "\n",
    "`TextLoader` is a tool designed to load textual data from various sources. It is the simplest loader, reading a file as text and placing all the content into a single document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b32604a-6fae-41ca-b02a-339e663a0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tCode of Conduct\n",
      "\n",
      "Our Code of Conduct outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability.\n",
      "Integrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest.\n",
      "Respect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy.\n",
      "Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of \n",
      "\n",
      "metadata: {'source': 'data/example_txt.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/example_txt.txt\")\n",
    "document_txt = loader.load()\n",
    "print(document_txt[0].page_content[:1000])\n",
    "print(f\"\\nmetadata: {document_txt[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147d457-bc2e-47b0-b2df-80bf73d30772",
   "metadata": {},
   "source": [
    "#### PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30a260-850c-4fe2-b99c-f7d852f4fa07",
   "metadata": {},
   "source": [
    "`PyPDFLoader` loads a PDF into an array of documents, where each document contains the page content and metadata with the page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b4895ec-0d7a-4e4a-b48f-491344ba9420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3 of 13\n",
      "Prinzi et al. European Radiology Experimental            (2024) 8:26 \n",
      " \n",
      "model validation are all steps that need to be addressed \n",
      "with high accuracy and robustness [11].\n",
      "This review aims to give primary educational insights \n",
      "on the most accessible and widely employed classifiers in \n",
      "radiology field discussing the following:\n",
      "• The main concepts related to the most widely used \n",
      "ML classifiers in the literature and their training\n",
      "• The main differences between shallow and deep \n",
      "learning classifiers, including the methods and the \n",
      "related feature extraction processes involved\n",
      "• Some practical guidelines on how to choose a clas -\n",
      "sifier, focusing mainly on data and computational \n",
      "resource availability, the task, and explainability \n",
      "requirements\n",
      "• The importance of explainable AI for the actual inte -\n",
      "gration of ML models in clinical practice\n",
      "Classifiers: main concepts\n",
      "Classification tasks aim to assign a class label to instances \n",
      "described by their respective features. These nu\n",
      "\n",
      "metadata: {'source': 'data/example_pdf.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/example_pdf.pdf\")\n",
    "document_pdf = loader.load()\n",
    "print(document_pdf[2].page_content[0:1000])\n",
    "print(f\"\\nmetadata: {document_pdf[2].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb924f-9fc2-4138-b2e0-cbcd0b95f4bd",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2995a4b-14e9-49bd-8970-d4c1497e8b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3 of 13\n",
      "Prinzi et al. European Radiology Experimental            (2024) 8:26 \n",
      "\t\n",
      "model validation are all steps that need to be addressed \n",
      "with high accuracy and robustness [11].\n",
      "This review aims to give primary educational insights \n",
      "on the most accessible and widely employed classifiers in \n",
      "radiology field discussing the following:\n",
      "•\t The main concepts related to the most widely used \n",
      "ML classifiers in the literature and their training\n",
      "•\t The main differences between shallow and deep \n",
      "learning classifiers, including the methods and the \n",
      "related feature extraction processes involved\n",
      "•\t Some practical guidelines on how to choose a clas-\n",
      "sifier, focusing mainly on data and computational \n",
      "resource availability, the task, and explainability \n",
      "requirements\n",
      "•\t The importance of explainable AI for the actual inte-\n",
      "gration of ML models in clinical practice\n",
      "Classifiers: main concepts\n",
      "Classification tasks aim to assign a class label to instances \n",
      "described by their respective features. These \n",
      "\n",
      "metadata: {'source': 'data/example_pdf.pdf', 'file_path': 'data/example_pdf.pdf', 'page': 2, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'Shallow and deep learning classifiers in medical image analysis', 'author': ' Francesco Prinzi ', 'subject': 'European Radiology Experimental, https://doi.org/10.1186/s41747-024-00428-2', 'keywords': 'Artificial intelligence;Explainable AI;Deep learning;Machine learning classifiers;Shallow learning', 'creator': 'Adobe InDesign 15.1 (Windows)', 'producer': 'Adobe PDF Library 15.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creationDate': 'D:20240215122431Z', 'modDate': \"D:20240216033816+01'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"data/example_pdf.pdf\")\n",
    "document_pdf = loader.load()\n",
    "print(document_pdf[2].page_content[0:1000])\n",
    "print(f\"\\nmetadata: {document_pdf[2].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7f6e2-ff3c-430c-8688-750eec735245",
   "metadata": {},
   "source": [
    "Another option is `PyPDFium2Loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b2991cd-17f6-49dd-a7ce-89cb899581da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prinzi et al. European Radiology Experimental (2024) 8:26 Page 3 of 13\n",
      "model validation are all steps that need to be addressed \n",
      "with high accuracy and robustness [11].\n",
      "Tis review aims to give primary educational insights \n",
      "on the most accessible and widely employed classifers in \n",
      "radiology feld discussing the following:\n",
      "• Te main concepts related to the most widely used \n",
      "ML classifers in the literature and their training\n",
      "• Te main diferences between shallow and deep \n",
      "learning classifers, including the methods and the \n",
      "related feature extraction processes involved\n",
      "• Some practical guidelines on how to choose a clas\u0002sifer, focusing mainly on data and computational \n",
      "resource availability, the task, and explainability \n",
      "requirements\n",
      "• Te importance of explainable AI for the actual inte\u0002gration of ML models in clinical practice\n",
      "Classifers: main concepts\n",
      "Classifcation tasks aim to assign a class label to instances \n",
      "described by their respective features. Tese numerical \n",
      "feat\n",
      "\n",
      "metadata: {'source': 'data/example_pdf.pdf', 'page': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(\"data/example_pdf.pdf\")\n",
    "document_pdf = loader.load()\n",
    "print(document_pdf[2].page_content[0:1000])\n",
    "print(f\"\\nmetadata: {document_pdf[2].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886cdcf-b322-482f-847e-3580e66d1c9a",
   "metadata": {},
   "source": [
    "#### Markdown files\n",
    "\n",
    "`UnstructuredMarkdownLoader` loads content from Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd458764-b387-49d2-b601-7a1e89908ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An h1 header\n",
      "\n",
      "Paragraphs are separated by a blank line.\n",
      "\n",
      "2nd paragraph. Italic, bold, and monospace. Itemized lists look like:\n",
      "\n",
      "this one\n",
      "\n",
      "that one\n",
      "\n",
      "the other one\n",
      "\n",
      "Note that --- not considering the asterisk --- the actual text content starts at 4-columns in.\n",
      "\n",
      "Block quotes are written like so.\n",
      "\n",
      "They can span multiple paragraphs, if you like.\n",
      "\n",
      "Use 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it's all in chapters 12--14\"). Three dots ... will be converted to an ellipsis. Unicode is suppor\n",
      "\n",
      "metadata: {'source': 'data/example_markdown.md'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"data/example_markdown.md\")\n",
    "document_markdown = loader.load()\n",
    "print(document_markdown[0].page_content[0:500])\n",
    "print(f\"\\nmetadata: {document_markdown[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba654e0a-6435-4bb6-b4a1-f55fc9d7d6a6",
   "metadata": {},
   "source": [
    "#### JSON files\n",
    "\n",
    "`JSONLoader` loads content from JSON files using a specified jq schema to parse the JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10f3a85e-ca5e-4af6-b2c1-32d655e3ff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought you were selling the blue one!\n",
      "\n",
      "metadata: {'source': '/Users/isabelle/Library/CloudStorage/OneDrive-Personal/OnlineCourses/IBM AI Engineering Certificate/Cheat_Sheets/data/example_json.json', 'seq_num': 4}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='data/example_json.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "document_json = loader.load()\n",
    "print(document_json[3].page_content)\n",
    "print(f\"\\nmetadata: {document_json[3].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b166e8d-cd9c-4171-adb0-85ce433edcc1",
   "metadata": {},
   "source": [
    "#### CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb683589-b749-4d96-93f8-c0c672201a31",
   "metadata": {},
   "source": [
    "`CSVLoader` loads tabular data from a csv file, treating each row as an individual document with headers defining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03669de7-516f-4b58-bcfa-11c32fdca1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: Nationals\n",
      "\"Payroll (millions)\": 81.34\n",
      "\"Wins\": 98\n",
      "\n",
      "metadata: {'source': 'data/example_csv.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\"data/example_csv.csv\")\n",
    "\n",
    "document_csv = loader.load()\n",
    "print(document_csv[0].page_content)\n",
    "print(f\"\\nmetadata: {document_csv[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271124c-c7a4-4cef-9982-32e28cbe66b5",
   "metadata": {},
   "source": [
    "`UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4d8c58b-cce0-46ef-8c87-20856498a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team \"Payroll (millions)\" \"Wins\" Nationals 81.34 98 Reds 82.20 97 Yankees 197.96 95 Giants 117.62 94 Braves 83.31 94 Athletics 55.37 94 Rangers 120.51 93 Orioles 81.43 93 Rays 64.17 90 Angels 154.49 89 Tigers 132.30 88 Cardinals 110.30 88 Dodgers 95.14 86 White Sox 96.92 85 Brewers 97.65 83 Phillies 174.54 81 Diamondbacks 74.28 81 Pirates 63.43 79 Padres 55.24 76 Mariners 81.97 75 Mets 93.35 74 Blue Jays 75.48 73 Royals 60.91 72 Marlins 118.07 69 Red Sox 173.18 69 Indians 78.43 68 Twins 94.08 66 Rockies 78.06 64 Cubs 88.19 61 Astros 60.65 55\n",
      "\n",
      "metadata: {'source': 'data/example_csv.csv'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
    "\n",
    "loader = UnstructuredCSVLoader(\"data/example_csv.csv\")\n",
    "\n",
    "document_csv = loader.load()\n",
    "print(document_csv[0].page_content)\n",
    "print(f\"\\nmetadata: {document_csv[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18718590-1544-48a2-9f49-bd72d12ce5b0",
   "metadata": {},
   "source": [
    "#### Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc7f6b61-c7f1-4074-a188-917f489b7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd party tutorialsYouTubearXivv0.2Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer \n",
      "\n",
      "metadata: {'source': 'https://python.langchain.com/v0.2/docs/introduction/', 'title': 'Introduction | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}\n",
      "\n",
      "numper of pages: 1\n",
      "\n",
      "\n",
      "s3rd party tutorialsYouTubearXivv0.2Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer\n",
      "\n",
      "metadata: {'source': 'https://python.langchain.com/v0.2/docs/how_to/', 'title': 'How-to guides | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Here you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.', 'language': 'en'}\n",
      "\n",
      "numper of pages: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\"\"\" Load from single web page \"\"\"\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "document_website = loader.load()\n",
    "print(document_website[0].page_content[200:1000])\n",
    "print(f\"\\nmetadata: {document_website[0].metadata}\")\n",
    "print(f\"\\nnumper of pages: {len(document_website)}\\n\\n\")\n",
    "\n",
    "\"\"\" Load from multiple web pages \"\"\"\n",
    "loader = WebBaseLoader([\"https://python.langchain.com/v0.2/docs/introduction/\", \"https://python.langchain.com/v0.2/docs/how_to/\"])\n",
    "document_websites = loader.load()\n",
    "print(document_websites[-1].page_content[200:1000])\n",
    "print(f\"\\nmetadata: {document_websites[-1].metadata}\")\n",
    "print(f\"\\nnumper of pages: {len(document_websites)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25c6d5-f303-40b7-a38c-99b2f76f6a47",
   "metadata": {},
   "source": [
    "#### Word files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ff0a85e-a446-465e-83da-f3a34b0e0d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration of DOCX support in calibre\n",
      "\n",
      "This document demonstrates the ability of the calibre DOCX Input plugin to convert the various typographic features in a Microsoft Word (2007 and newer) document. Convert this document to a modern ebook format, such as AZW3 for Kindles or EPUB for other ebook readers, to see it in action.\n",
      "\n",
      "There is support for images, tables, lists, footnotes, endnotes, links, dropcaps and various types of text and paragraph level formatting.\n",
      "\n",
      "To see the DOCX conversion \n",
      "\n",
      "metadata: {'source': 'data/example_word.docx'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"data/example_word.docx\")\n",
    "document_word = loader.load()\n",
    "print(document_word[0].page_content[0:500])\n",
    "print(f\"\\nmetadata: {document_word[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b0a84-a63f-48f4-b721-db6214fce61e",
   "metadata": {},
   "source": [
    "#### Unstructured files\n",
    "\n",
    "Sometimes, we need to load content from various text sources and formats without writing a separate loader for each one. Additionally, when a new file format emerges, we want to save time by not having to write a new loader for it. `UnstructuredFileLoader` addresses this need by supporting the loading of multiple file types. Currently, `UnstructuredFileLoader` can handle text files, PowerPoints, HTML, PDFs, images, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "056a2161-b250-449d-a979-8cb522307b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/ipykernel_41042/211387894.py:5: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the langchain-unstructured package and should be used instead. To use it run `pip install -U langchain-unstructured` and import as `from langchain_unstructured import UnstructuredLoader`.\n",
      "  loader = UnstructuredFileLoader(\"data/example_txt.txt\")\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "metadata: {'source': 'data/example_txt.txt'}\n",
      "\n",
      "metadata: {'source': ['data/example_txt.txt', 'data/example_markdown.md']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "\"\"\" Load from single file \"\"\"\n",
    "\n",
    "loader = UnstructuredFileLoader(\"data/example_txt.txt\")\n",
    "document_txt_unstructured = loader.load()\n",
    "# print(document_txt_unstructured[0].page_content)\n",
    "print(f\"\\nmetadata: {document_txt_unstructured[0].metadata}\")\n",
    "\n",
    "\"\"\" Load from multiple files with different formats \"\"\"\n",
    "\n",
    "loader = UnstructuredFileLoader([\"data/example_txt.txt\", \"data/example_markdown.md\"])\n",
    "document_multiple_unstructured = loader.load()\n",
    "# print(document_multiple_unstructured[0].page_content)\n",
    "print(f\"\\nmetadata: {document_multiple_unstructured[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af77c51-607e-41de-b34d-48fc7b325bb8",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "LLMs often have a context window of a certain length. Thus, documents longer than this length will be cut off, meaning that crucial information may be lost. Thus, documents may need to be split into smaller chunks.\n",
    "\n",
    "At a high level, text splitters work as follows:\n",
    "\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d2215-7755-4915-add3-1fbda38f8aa1",
   "metadata": {},
   "source": [
    "#### Split by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33945829-85f0-4af4-86c2-81c936e184e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\"\"\" Split at any character \"\"\"\n",
    "text_splitter_character = CharacterTextSplitter(chunk_size=500, chunk_overlap=20, separator=\"\", length_function=len)  \n",
    "\n",
    "\"\"\" Split between sentences (\".\") \"\"\"\n",
    "text_splitter_sentence = CharacterTextSplitter(chunk_size=500, chunk_overlap=20, separator=\".\", length_function=len)\n",
    "\n",
    "\"\"\" Split at line breaks (\"\\n\") \"\"\"\n",
    "text_splitter_lb = CharacterTextSplitter(chunk_size=500, chunk_overlap=20, separator=\"\\n\", length_function=len)  \n",
    "\n",
    "\"\"\" Split between paragraphs (\"\\n\\n\") \"\"\"\n",
    "text_splitter_paragraph = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\"\\n\\n\", length_function=len) #default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d182241e-3c52-4f24-bfbf-5a8a7f49a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1624, which is longer than the specified 1000\n",
      "Created a chunk of size 1885, which is longer than the specified 1000\n",
      "Created a chunk of size 1903, which is longer than the specified 1000\n",
      "Created a chunk of size 1729, which is longer than the specified 1000\n",
      "Created a chunk of size 1678, which is longer than the specified 1000\n",
      "Created a chunk of size 2032, which is longer than the specified 1000\n",
      "Created a chunk of size 1894, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length (page numbers): 1\n",
      "Split document length: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6.\\tDrug and Alcohol Policy'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_txt_chunks = text_splitter_paragraph.split_documents(document_txt)\n",
    "\n",
    "print(f\"Original document length (page numbers): {len(document_txt)}\")\n",
    "print(f\"Split document length: {len(document_txt_chunks)}\")\n",
    "\n",
    "document_txt_chunks[10].page_content   # take a look at any chunk's page content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127b0e5-f43b-4bbc-8a12-f982732920f0",
   "metadata": {},
   "source": [
    "#### Recursively split by character\n",
    "\n",
    "This text splitter is parameterised by a list of characters, and it tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c39161-6533-42b5-aae8-1e65ed1faf17",
   "metadata": {},
   "source": [
    "##### Default separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca215f9d-07aa-44cf-bf0d-7f22306f7381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length (page numbers): 13\n",
      "Split document length: 879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, length_function=len)\n",
    "\n",
    "document_pdf_chunks = text_splitter_recursive.split_documents(document_pdf)\n",
    "\n",
    "print(f\"Original document length (page numbers): {len(document_pdf)}\")\n",
    "print(f\"Split document length: {len(document_pdf_chunks)}\")\n",
    "\n",
    "document_pdf_chunks[4].page_content   # take a look at any chunk's page content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb9611-c3c6-4c78-bbf9-e913711a655e",
   "metadata": {},
   "source": [
    "##### Custom separators for different programming languages\n",
    "\n",
    "The list of characters used for splitting can be customised based on the document's content. For documents containing code, the `Language` package can be imported to use recommended separators for different programming languages. \n",
    "Note that you need to call `.from_language` after the `RecursiveCharacterTextSplitter` and specify the `language`. The other parameter settings remain the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c02af8d-959c-4319-8c8d-e9469e399d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages: ['cpp', 'go', 'java', 'kotlin', 'js', 'ts', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html', 'sol', 'csharp', 'cobol', 'c', 'lua', 'perl', 'haskell', 'elixir', 'powershell']\n",
      "\n",
      "Separators used for python: ['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\n",
      "Python example chunk: print(\"Hello, World!\")\n",
      "\n",
      "Separators used for JSON: ['\\nfunction ', '\\nconst ', '\\nlet ', '\\nvar ', '\\nclass ', '\\nif ', '\\nfor ', '\\nwhile ', '\\nswitch ', '\\ncase ', '\\ndefault ', '\\n\\n', '\\n', ' ', '']\n",
      "JSON example chunk: console.log(\"Hello, World!\");\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "print(f\"Available languages: {[e.value for e in Language]}\\n\")\n",
    "\n",
    "\"\"\" Python \"\"\"\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "    def hello_world():\n",
    "        print(\"Hello, World!\")\n",
    "    \n",
    "    # Call the function\n",
    "    hello_world()\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Separators used for python: {RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)}\")\n",
    "text_splitter_python = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=50, chunk_overlap=0)\n",
    "document_python_chunks = text_splitter_python.create_documents([PYTHON_CODE])\n",
    "\n",
    "print(f\"Python example chunk: {document_python_chunks[1].page_content}\\n\")\n",
    "\n",
    "\n",
    "\"\"\" Javascript \"\"\"\n",
    "\n",
    "JS_CODE = \"\"\"\n",
    "    function helloWorld() {\n",
    "      console.log(\"Hello, World!\");\n",
    "    }\n",
    "    \n",
    "    // Call the function\n",
    "    helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Separators used for JSON: {RecursiveCharacterTextSplitter.get_separators_for_language(Language.JS)}\")\n",
    "text_splitter_json = RecursiveCharacterTextSplitter.from_language(language=Language.JS, chunk_size=50, chunk_overlap=0)\n",
    "document_json_chunks = text_splitter_json.create_documents([JS_CODE])\n",
    "\n",
    "print(f\"JSON example chunk: {document_json_chunks[1].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c345b53-ec74-4fe2-ba24-ece5d532343e",
   "metadata": {},
   "source": [
    "#### Split markdown file by headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598ea8c-cbb2-46ab-9226-78259f64989c",
   "metadata": {},
   "source": [
    "`MarkdownHeaderTextSplitter` will divide a Markdown file based on a specified set of headers, ensuring that text with a common context remains together.\n",
    "\n",
    "If you want the headers appears in the page_content as well, you can specify `strip_headers=False` when you call the `MarkdownHeaderTextSplitter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "356bac13-0acf-4bc4-9bb4-00f497dc678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'An h1 header #'}, page_content='Paragraphs are separated by a blank line.  \\n2nd paragraph. *Italic*, **bold**, and `monospace`. Itemized lists\\nlook like:  \\n* this one\\n* that one\\n* the other one  \\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.  \\n> Block quotes are\\n> written like so.\\n>\\n> They can span multiple paragraphs,\\n> if you like.  \\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺'),\n",
       " Document(metadata={'Header 1': 'An h1 header #', 'Header 2': 'An h2 header ##'}, page_content=\"Here's a numbered list:  \\n1. first item\\n2. second item\\n3. third item  \\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here's a code sample:\"),\n",
       " Document(metadata={'Header 1': 'Let me re-iterate ...'}, page_content='for i in 1 .. 10 { do-something(i) }  \\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:  \\n~~~\\ndefine foobar() {\\nprint \"Welcome to flavor country!\";\\n}\\n~~~  \\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:  \\n~~~python\\nimport time\\n# Quick, count to ten!\\nfor i in range(10):\\n# (but not *too* quick)\\ntime.sleep(0.5)\\nprint i\\n~~~'),\n",
       " Document(metadata={'Header 1': 'Let me re-iterate ...', 'Header 3': 'An h3 header ###'}, page_content='Now a nested list:  \\n1. First, get these ingredients:  \\n* carrots\\n* celery\\n* lentils  \\n2. Boil some water.  \\n3. Dump everything in the pot and follow\\nthis algorithm:  \\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)  \\nDo not bump wooden spoon or it will fall.  \\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).  \\nHere\\'s a link to [a website](http://foo.bar), to a [local\\ndoc](local-doc.html), and to a [section heading in the current\\ndoc](#an-h2-header). Here\\'s a footnote [^1].  \\n[^1]: Footnote text goes here.  \\nTables can look like this:  \\nsize  material      color\\n----  ------------  ------------\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent  \\nTable: Shoes, their sizes, and what they\\'re made of  \\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:  \\n--------  -----------------------\\nkeyword   text\\n--------  -----------------------\\nred       Sunsets, apples, and\\nother red or reddish\\nthings.  \\ngreen     Leaves, grass, frogs\\nand other things it\\'s\\nnot easy being.\\n--------  -----------------------  \\nA horizontal rule follows.  \\n***  \\nHere\\'s a definition list:  \\napples\\n: Good for making applesauce.\\noranges\\n: Citrus!\\ntomatoes\\n: There\\'s no \"e\" in tomatoe.  \\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)  \\nHere\\'s a \"line block\":  \\n| Line one\\n|   Line too\\n| Line tree  \\nand images can be specified like so:  \\n![example image](example-image.jpg \"An exemplary image\")  \\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:  \\n$$I = \\\\int \\\\rho R^{2} dV$$  \\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: \\\\`foo\\\\`, \\\\*bar\\\\*, etc.')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = TextLoader(\"data/example_markdown.md\")\n",
    "document_markdown_astext = loader.load()\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter_markdown = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "document_markdown_chunks = text_splitter_markdown.split_text(document_markdown_astext[0].page_content)\n",
    "document_markdown_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85e3ed-3e88-40fb-a908-51c22d52734a",
   "metadata": {},
   "source": [
    "#### Split HTML file by headers/sections\n",
    "\n",
    "The `HTMLHeaderTextSplitter` will divide an HTML file based on a specified set of headers, splitting text at the element level and adding metadata for each header \"relevant\" to any given chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ffc3611-27e3-41de-8aec-6144a998ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>Foo</h1>\n",
    "            <p>Some intro text about Foo.</p>\n",
    "            <div>\n",
    "                <h2>Bar main section</h2>\n",
    "                <p>Some intro text about Bar.</p>\n",
    "                <h3>Bar subsection 1</h3>\n",
    "                <p>Some text about the first subtopic of Bar.</p>\n",
    "                <h3>Bar subsection 2</h3>\n",
    "                <p>Some text about the second subtopic of Bar.</p>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h2>Baz</h2>\n",
    "                <p>Some text about Baz</p>\n",
    "            </div>\n",
    "            <br>\n",
    "            <p>Some concluding text about Foo</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e77020d3-3e86-4583-a58c-dcaf1ab7fff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Foo'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Some intro text about Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Some text about the first subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Baz'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Some text about Baz'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some concluding text about Foo')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter_html = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "document_html_chunks = text_splitter_html.split_text(html_string)\n",
    "document_html_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a720d0f-df32-418b-890e-be3403eb3b5d",
   "metadata": {},
   "source": [
    "The `HTMLSectionSplitter` is also a \"structure-aware\" chunker that splits text section by section based on headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b0acccd-5529-4a13-88ec-cbab0b335230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo'}, page_content='Foo \\n Some intro text about Foo.'),\n",
       " Document(metadata={'Header 2': 'Bar main section'}, page_content='Bar main section \\n Some intro text about Bar.'),\n",
       " Document(metadata={'Header 3': 'Bar subsection 1'}, page_content='Bar subsection 1 \\n Some text about the first subtopic of Bar.'),\n",
       " Document(metadata={'Header 3': 'Bar subsection 2'}, page_content='Bar subsection 2 \\n Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 2': 'Baz'}, page_content='Baz \\n Some text about Baz \\n \\n \\n Some concluding text about Foo')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter_html = HTMLSectionSplitter(headers_to_split_on=headers_to_split_on)\n",
    "document_html_chunks = text_splitter_html.split_text(html_string)\n",
    "document_html_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed6b57-2ac4-47b3-9b27-4d54d4790eb9",
   "metadata": {},
   "source": [
    "### Embedding models \n",
    "\n",
    "Embeddings generate a vector representation for a given piece of text. This is advantageous as it allows you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36e830-dffb-43f5-b1a2-5357a0a36e21",
   "metadata": {},
   "source": [
    "#### Watsonx embedding model\n",
    "The `slate-125m-english-rtrvr` embedding model, formerly known as WatBERT, has the same architecture as a RoBERTa base transformer model and has ~125 million parameters and an embedding dimension of 768. It is a standard sentence transformer model based on bi-encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bd810ad-d59d-4b49-846e-89a8ee4aaadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [-0.06722454, -0.023729993, 0.017487843, -0.013195328, -0.039584607]\n",
      "Document embedding example: [-0.04865915, 0.026073253, -0.03460521, -0.01589293, -0.022831136]\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "import json\n",
    "\n",
    "def WatsonxEmbedding(model_id, params=None):\n",
    "    \n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    \n",
    "    apikey_filename=\"data/watsonx_apikey.json\"\n",
    "    data = json.load(open(apikey_filename, 'r'))\n",
    "    credentials = {\n",
    "                \"url\": \"https://eu-gb.ml.cloud.ibm.com\", \n",
    "                \"apikey\": data.get(\"apikey\")\n",
    "            }\n",
    "    \n",
    "    project_id = data.get(\"project_id\")\n",
    "    \n",
    "    embedding = WatsonxEmbeddings(\n",
    "        model_id=model_id,\n",
    "        params=embed_params,\n",
    "        url=credentials[\"url\"],\n",
    "        apikey=credentials[\"apikey\"], # Use 'apikey' as the parameter name\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\"\"\" Create embedding model \"\"\"\n",
    "# model_id = \"ibm/slate-30m-english-rtrvr\"\n",
    "model_id=\"ibm/slate-125m-english-rtrvr\"\n",
    "watsonx_embedding = WatsonxEmbedding(model_id)\n",
    "\n",
    "\"\"\" Embed query \"\"\"\n",
    "query = \"How are you?\"\n",
    "query_result = watsonx_embedding.embed_query(query)\n",
    "print(f\"Query embedding: {query_result[:5]}\")\n",
    "\n",
    "\"\"\" Embed document \"\"\"\n",
    "texts = [text.page_content for text in document_pdf_chunks[0:10]]\n",
    "embedding_result = watsonx_embedding.embed_documents(texts)\n",
    "embedding_result[0][:5]\n",
    "print(f\"Document embedding example: {embedding_result[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba55a9-4f86-4bd5-8a74-b1dace30f4da",
   "metadata": {},
   "source": [
    "#### HuggingFace embedding model\n",
    "\n",
    "The `all-mpnet-base-v2` from HuggingFace is a sentence-transformer model. It maps sentences and paragraphs to a 768-dimensional dense vector space and can be used for tasks like clustering or semantic search. It used the pre-trained `Microsoft/money-base` model and fine-tuned it on a 1B sentence pairs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbfd5245-a4e6-4aed-aeef-d3f7f096c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/ipykernel_41042/2102212079.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  huggingface_embedding = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [0.027106232941150665, 0.011331922374665737, -0.0019524093950167298, -0.03695136308670044, 0.017764894291758537]\n",
      "Document embedding example: [0.028991112485527992, -0.06590613722801208, -0.02160440944135189, 0.013643010519444942, -0.04726509004831314]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\"\"\" Create embedding model \"\"\"\n",
    "huggingface_embedding = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\"\"\" Embed query \"\"\"\n",
    "query = \"How are you?\"\n",
    "query_result = huggingface_embedding.embed_query(query)\n",
    "print(f\"Query embedding: {query_result[:5]}\")\n",
    "\n",
    "\"\"\" Embed document \"\"\"\n",
    "texts = [text.page_content for text in document_pdf_chunks[0:10]]\n",
    "embedding_result = huggingface_embedding.embed_documents(texts)\n",
    "print(f\"Document embedding example: {embedding_result[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dd1de-bce8-4ad5-ba76-8e0355fca88e",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b712e48-2f84-4e77-92f6-946cf2b9a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_pdf_chunks_small=document_pdf_chunks[0:100]\n",
    "document_txt_chunks_small=document_txt_chunks[0:10]\n",
    "pdf_chunks_ids = [str(i) for i in range(0, len(document_pdf_chunks_small))]\n",
    "txt_chunks_ids = [str(i) for i in range(0, len(document_txt_chunks_small))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8e274-4826-48aa-9e00-e2c39ef54104",
   "metadata": {},
   "source": [
    "#### Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "470d5707-4599-47f2-9462-89c07c459532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store_chroma_pdf = Chroma.from_documents(document_pdf_chunks_small, \n",
    "                                                watsonx_embedding, \n",
    "                                                ids=pdf_chunks_ids, \n",
    "                                                persist_directory=\"./data/chroma_pdf_db\")\n",
    "\n",
    "# vector_store_chroma_txt = Chroma.from_documents(document_txt_chunks_small, \n",
    "#                                                 huggingface_embedding, \n",
    "#                                                 ids=txt_chunks_ids, \n",
    "#                                                 persist_directory=\"./data/chroma_txt_db\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df263f8c-e15f-43e8-b5b8-f48b4d177e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vector database (number of chunks): 104\n",
      "\n",
      "{'ids': ['0'], 'embeddings': None, 'documents': ['Prinzi\\xa0et\\xa0al. European Radiology Experimental (2024) 8:26'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'source': 'data/example_pdf.pdf', 'page': 0}]}\n",
      "{'ids': ['1'], 'embeddings': None, 'documents': ['https://doi.org/10.1186/s41747-024-00428-2\\r\\nNARRATIVE REVIEW Open Access'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'page': 0, 'source': 'data/example_pdf.pdf'}]}\n",
      "{'ids': ['2'], 'embeddings': None, 'documents': ['© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'source': 'data/example_pdf.pdf', 'page': 0}]}\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the vector database (number of chunks): {vector_store_chroma_pdf._collection.count()}\\n')\n",
    "for i in range(3):\n",
    "    print(vector_store_chroma_pdf._collection.get(ids=str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5f3ac-1e18-4be6-93d6-caaf557a5fe5",
   "metadata": {},
   "source": [
    "#### FAISS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a877ac4-ad4e-496c-a1b5-981e206aee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store_faiss_pdf = FAISS.from_documents(document_pdf_chunks_small, \n",
    "                                              watsonx_embedding, \n",
    "                                              ids=pdf_chunks_ids)\n",
    "\n",
    "vector_store_faiss_txt = FAISS.from_documents(document_txt_chunks_small, \n",
    "                                              watsonx_embedding, \n",
    "                                              ids=txt_chunks_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae3d5697-dfec-451c-b363-42638e404f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vector database (number of chunks): 100\n",
      "\n",
      "page_content='Prinzi et al. European Radiology Experimental (2024) 8:26' metadata={'source': 'data/example_pdf.pdf', 'page': 0}\n",
      "page_content='https://doi.org/10.1186/s41747-024-00428-2\n",
      "NARRATIVE REVIEW Open Access' metadata={'source': 'data/example_pdf.pdf', 'page': 0}\n",
      "page_content='© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0' metadata={'source': 'data/example_pdf.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the vector database (number of chunks): {vector_store_faiss_pdf.index.ntotal}\\n')\n",
    "for i in range(3):\n",
    "    print(vector_store_faiss_pdf.docstore.search(str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82053a01-9537-4884-8329-171e5a6debaf",
   "metadata": {},
   "source": [
    "#### Edit vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93845fdb-1411-4740-93f6-f9da5cb4ce8c",
   "metadata": {},
   "source": [
    "##### Add entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4a83771-5583-468d-88ab-b4151ebdbe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['104', '105']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text1 = \"This is the first sample chunk to add to the vector store.\"\n",
    "text2 = \"This is the second sample chunk to add to the vector store.\"\n",
    "\n",
    "new_chunk1 =  Document(page_content=text1, metadata={\"source\": \"source.com\", \"page\": 1})\n",
    "new_chunk2 =  Document(page_content=text2, metadata={\"source\": \"source.com\", \"page\": 1})\n",
    "\n",
    "new_chunks = [new_chunk1, new_chunk2]\n",
    "vector_store_len = vector_store_chroma_pdf._collection.count()\n",
    "new_ids = [str(i) for i in range(vector_store_len, vector_store_len+len(new_chunks))]\n",
    "\n",
    "vector_store_chroma_pdf.add_documents(new_chunks, ids=new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28ea7c3a-a05d-4729-b62b-099e05d22df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['100'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['This is the first sample chunk to add to the vector store.'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'source.com', 'page': 1}]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_chroma_pdf._collection.get(ids='100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37120622-d5f3-4849-85dd-f389ffe8a427",
   "metadata": {},
   "source": [
    "##### Update entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3320a776-5bb1-4028-b2cd-6b6d64c6f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['55'], 'embeddings': None, 'documents': ['This is a sample chunk that will replace an entry within the vector store.'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'source': 'source.com', 'page': 1}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text3 = \"This is a sample chunk that will replace an entry within the vector store.\"\n",
    "\n",
    "new_chunk3 =  Document(page_content=text3, metadata={\"source\": \"source.com\", \"page\": 1})\n",
    "\n",
    "vector_store_chroma_pdf.update_document('55', new_chunk3)\n",
    "\n",
    "print(vector_store_chroma_pdf._collection.get(ids=['55']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e82e95-4cf0-4806-9a54-9c48fe9cfdb0",
   "metadata": {},
   "source": [
    "##### Delete entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2539556d-03ee-4a2e-a230-b131f4de5ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [], 'embeddings': None, 'documents': [], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': []}\n"
     ]
    }
   ],
   "source": [
    "vector_store_chroma_pdf._collection.delete(ids=['22'])\n",
    "print(vector_store_chroma_pdf._collection.get(ids=['22']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cc774-873d-4dd2-b60d-0de98814aa78",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "448fa6f9-6a30-46cf-b764-934d1d4bb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"shallow learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ec85c-9b5e-428d-89de-63ef8c6e4fb6",
   "metadata": {},
   "source": [
    "#### Use 'similarity_search' method of the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8008cd34-b72a-4352-b30f-28e1661a9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma search result: shallow neural networks (neural networks with only one\n",
      "FAISS search result: shallow neural networks (neural networks with only one\n"
     ]
    }
   ],
   "source": [
    "search_result_chroma = vector_store_chroma_pdf.similarity_search(query) #add k = 1 to just retrieve the top one result\n",
    "print(f\"Chroma search result: {search_result_chroma[0].page_content}\")\n",
    "\n",
    "search_result_faiss = vector_store_faiss_pdf.similarity_search(query) #add k = 1 to just retrieve the top one result\n",
    "print(f\"FAISS search result: {search_result_faiss[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620f5d1-3ead-4382-a0b3-af7ac745ec3c",
   "metadata": {},
   "source": [
    "#### Vector store backed retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff619b3f-b05f-4d89-a5b8-5721939dec73",
   "metadata": {},
   "source": [
    "##### Simple similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b960cc93-8313-469f-a2fc-ba9e1e07fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow neural networks (neural networks with only one\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store_chroma_pdf.as_retriever() #to limit retrieval to top k results, add search_kwargs={\"k\": 2}\n",
    "search_result = retriever.invoke(query)\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2edfa5-2d45-41ba-9183-8a5a31fcd6e6",
   "metadata": {},
   "source": [
    "##### MMR retrieval\n",
    "\n",
    "MMR in vector stores is a technique used to balance the relevance and diversity of retrieved results. It selects documents that are both highly relevant to the query and minimally similar to previously selected documents. This approach helps to avoid redundancy and ensures a more comprehensive coverage of different aspects of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "add1309e-5ba2-42fb-95f8-2f75ddca141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow neural networks (neural networks with only one\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store_chroma_pdf.as_retriever(search_type=\"mmr\")\n",
    "search_result = retriever.invoke(query)\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb2832-a51c-4051-b4e7-fb84cf0dfe3c",
   "metadata": {},
   "source": [
    "##### Similarity score threshold retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aecb83a7-d17b-48cd-b0d7-7fc6ff875439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow neural networks (neural networks with only one\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store_chroma_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4})\n",
    "search_result = retriever.invoke(query)\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4dae4-a354-4afa-a721-846cf9d12c6f",
   "metadata": {},
   "source": [
    "#### Multi-query retriever\n",
    "\n",
    "The `MultiQueryRetriever` uses an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and then takes the unique union of these results to form a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` can potentially overcome some limitations of distance-based retrieval, resulting in a richer and more diverse set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbcc1928-f4c3-4ac5-94e2-185d2d580708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Can you provide a summary of the paper's discussion on shallow learning techniques?\", '2. What are the key points made in the paper regarding shallow learning methodologies?', '3. In what ways does the paper explore or examine shallow learning approaches?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could be a normal or abnormal tissue, a vessel, a tumor,\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import logging\n",
    "\n",
    "query = \"What does the paper say about shallow learning?\"\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(retriever=vector_store_chroma_pdf.as_retriever(), llm=mixtral_llm)\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "search_result = retriever.invoke(query)\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4bac4-177e-4efb-b036-8b8b8eeb4a79",
   "metadata": {},
   "source": [
    "#### Self-querying retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3cf82a8e-be17-43cb-8661-b9b8283e23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "document_movies = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of a movie.\"\n",
    "\n",
    "vector_store_chroma_movies = Chroma.from_documents(document_movies, watsonx_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e6515e6-f6c7-4e2c-adda-9e7bc0c9de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three men walk into the Zone, three men walk out of the Zone\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "import lark \n",
    "\n",
    "query = \"I want to watch a movie rated higher than 8.5\"\n",
    "retriever = SelfQueryRetriever.from_llm(mixtral_llm, vector_store_chroma_movies, document_content_description, metadata_field_info)\n",
    "search_result = retriever.invoke(query)\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47cede1-0db2-4654-83e3-15f5e49411e0",
   "metadata": {},
   "source": [
    "#### Parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3c50cfd-4f66-4f2f-8ed9-19f8f06cb867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/ipykernel_41042/428405337.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_store = Chroma(collection_name=\"split_parents\", embedding_function=watsonx_embedding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the material. If material is not included in the article’s Creative Commons licence and your\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "parent_text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "vector_store = Chroma(collection_name=\"split_parents\", embedding_function=watsonx_embedding)\n",
    "store = InMemoryStore() # The storage layer for the parent documents\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    child_splitter=text_splitter_recursive,\n",
    "    parent_splitter=parent_text_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(document_pdf_chunks[0:100])\n",
    "search_result = retriever.invoke(query) # retrieve the relevant large chunk\n",
    "print(search_result[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985da3c0-e13c-49ad-bd40-8b113118c209",
   "metadata": {},
   "source": [
    "#### RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0dde7674-ef15-41a1-96d6-9f6a1a9010aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is this paper discussing?', 'result': \" Based on the provided context, it appears that this text is discussing the process of adding and replacing entries within a vector store. However, there is no specific paper mentioned in the context, so it's unclear which paper the question is referring to.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever=vector_store_chroma_pdf.as_retriever()\n",
    "chain = RetrievalQA.from_chain_type(llm=mixtral_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=retriever, \n",
    "                                 return_source_documents=False)\n",
    "query = \"what is this paper discussing?\"\n",
    "search_result=chain.invoke(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8edb54a6-bf5f-4a79-81cd-bb98de730aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flan-ul2 model \n",
      "Query: What is mobile policy? \n",
      "Response: The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations. Security: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device. Confidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces. Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones. Compliance: Adhere to all pertinent laws and regulations concerning mobile phone usage, including those related to data protection and privacy. Lost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor. Consequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.\n",
      "\n",
      "Flan-ul2 model \n",
      "Query: Can you summarize the document for me? \n",
      "Response: Code of Conduct, Mobile Phone Policy, Smoking Policy, Recruitment Policy.. I think that's it.. I'm not sure.. I'm not sure what the document is about.. I'm not sure what the question is.. I'm not sure what the answer is.. I'm not sure what the question is.. I'm not sure what the answer is.. I'm not sure what the question is.. I'm not sure what the question is.. I'm not sure what the question is.. I'm not sure what the question is..\n",
      "\n",
      "Llama model \n",
      "Query: Can you summarize the document for me? \n",
      "Response:  I don't know. The text appears to be a list of policy titles, but there is no actual content or information provided about the policies themselves. \n",
      "Unhelpful Answer: The document is about a company's rules and regulations. \n",
      "\n",
      "Note: The answer should be the following: I don't know. \n",
      "\n",
      "I don't know.\n",
      "\n",
      "Llama model with prompt template \n",
      "Query: Can I eat in company vehicles? \n",
      "Response: The document does not mention anything about eating in company vehicles. \n",
      "# I don't know\n",
      "The best answer is I don't know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever=vector_store_faiss_txt.as_retriever()\n",
    "\n",
    "chain_flan_ul2_llm = RetrievalQA.from_chain_type(llm=flan_ul2_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=retriever, \n",
    "                                 return_source_documents=False)\n",
    "\n",
    "chain_llama_llm = RetrievalQA.from_chain_type(llm=llama_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=retriever, \n",
    "                                 return_source_documents=False)\n",
    "\n",
    "query1 = \"What is mobile policy?\"\n",
    "query2 = \"Can you summarize the document for me?\"\n",
    "\n",
    "search_result1=chain_flan_ul2_llm.invoke(query1)\n",
    "search_result2=chain_flan_ul2_llm.invoke(query2)\n",
    "search_result3=chain_llama_llm.invoke(query2)\n",
    "\n",
    "print(f\"Flan-ul2 model \\nQuery: {query1} \\nResponse: {search_result1['result']}\\n\")\n",
    "print(f\"Flan-ul2 model \\nQuery: {query2} \\nResponse: {search_result2['result']}\\n\")\n",
    "print(f\"Llama model \\nQuery: {query2} \\nResponse: {search_result3['result']}\\n\")\n",
    "\n",
    "prompt_template = \"\"\"Use the information from the document to answer the question at the end. If you don't know the answer, just say that you don't know, do not try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "#When you set chain_type=\"stuff\", it means that all of the retrieved documents is passed as \"context\" and the query is passed as \"question\".\n",
    "chain_type_kwargs = {\"prompt\": PROMPT} \n",
    "\n",
    "chain_llama_llm_with_prompt_template = RetrievalQA.from_chain_type(llm=llama_llm, \n",
    "                                                                   chain_type=\"stuff\", \n",
    "                                                                   retriever=retriever, \n",
    "                                                                   chain_type_kwargs=chain_type_kwargs,\n",
    "                                                                   return_source_documents=False)\n",
    "\n",
    "query3 = \"Can I eat in company vehicles?\"\n",
    "search_result4=chain_llama_llm_with_prompt_template.invoke(query3)\n",
    "print(f\"Llama model with prompt template \\nQuery: {query3} \\nResponse: {search_result4['result']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289ffb3-e395-47aa-8d76-8e531b6f805a",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b0305ae-c37d-4929-a9ef-a61c2b078baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.local/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:370: LifecycleWarning: Model 'mistralai/mixtral-8x7b-instruct-v01' is in deprecated state from 2025-04-30 until 2025-07-30. IDs of alternative models: mistralai/mistral-small-3-1-24b-instruct-2503. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 50,  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: 10, # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: 0.2,\n",
    "        GenParams.TOP_K: 1\n",
    "}\n",
    "\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "mixtral_llm = watsonx_model(model_id, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762c977-b7f5-48bc-95aa-e8562da2778a",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cfcbfb6b-3f7d-49ea-a460-ebbe0fe0c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: [AIMessage(content='hi!'), HumanMessage(content='what is the capital of France?')]\n",
      "\n",
      "response: \n",
      "AI: The capital of France is Paris. Would you like to know about the history of Paris?\n",
      "\n",
      "new chat history: [AIMessage(content='hi!'), HumanMessage(content='what is the capital of France?'), AIMessage(content='\\nAI: The capital of France is Paris. Would you like to know about the history of Paris?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"hi!\")\n",
    "history.add_user_message(\"what is the capital of France?\")\n",
    "\n",
    "prompt = history.messages\n",
    "response = mixtral_llm.invoke(prompt)\n",
    "\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")\n",
    "\n",
    "history.add_ai_message(response)\n",
    "print(f\"new chat history: {history.messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8c3d9-7030-45cf-b3f6-55f21f6e3af6",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a9e020a4-f8c7-460b-8446-379534e8132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Hello, I am a little cat. Who are you?', 'history': '', 'response': \" Hello, I am an AI language model. I am a computer program that can understand and generate human-like text based on the input I receive. I don't have a physical form, so I can't be a cat, but I\"}\n",
      "{'input': 'What can you do?', 'history': \"Human: Hello, I am a little cat. Who are you?\\nAI:  Hello, I am an AI language model. I am a computer program that can understand and generate human-like text based on the input I receive. I don't have a physical form, so I can't be a cat, but I\", 'response': \" I can answer questions, write essays, summarize texts, translate languages, generate creative ideas, and much more. I can access a vast amount of information on the internet and use it to provide detailed and accurate responses. However, I don't\"}\n",
      "{'input': 'Who am I?', 'history': \"Human: Hello, I am a little cat. Who are you?\\nAI:  Hello, I am an AI language model. I am a computer program that can understand and generate human-like text based on the input I receive. I don't have a physical form, so I can't be a cat, but I\\nHuman: What can you do?\\nAI:  I can answer questions, write essays, summarize texts, translate languages, generate creative ideas, and much more. I can access a vast amount of information on the internet and use it to provide detailed and accurate responses. However, I don't\", 'response': '  Based on the input you provided, you are a cat. More specifically, you are a \"little cat.\" However, I don\\'t have the ability to truly know who or what you are, as I am a computer program and do'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=mixtral_llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response1 = conversation.invoke(input=\"Hello, I am a little cat. Who are you?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = conversation.invoke(input=\"What can you do?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = conversation.invoke(input=\"Who am I?\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7bf6bc5-36d5-484c-ac8b-f3f84b17e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. \n",
      "\n",
      "In more detail, the policy includes guidelines on acceptable use, security, confidentiality, cost management, compliance, lost or stolen devices, and consequences for non-compliance. \n",
      "\n",
      "The policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices. \n",
      "\n",
      "The policy can be summarized as follows: \n",
      "- Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\n",
      "- Security: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\n",
      "- Confidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.\n",
      "- Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/ipykernel_41042/666069523.py:19: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  response = conversation({\"question\": query}, {\"chat_history\": history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know. \n",
      "(There is no mention of a mobile policy in the provided context.)\n",
      " I don't know. \n",
      "(There is no mention of a mobile policy in the provided context.)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever=vector_store_faiss_txt.as_retriever()\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n",
    "\n",
    "conversation = ConversationalRetrievalChain.from_llm(llm=llama_llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=retriever, \n",
    "                                           memory = memory, \n",
    "                                           get_chat_history=lambda h : h, \n",
    "                                           return_source_documents=False)\n",
    "history = []\n",
    "query = \"What is mobile policy?\"\n",
    "response = conversation.invoke({\"question\":query}, {\"chat_history\": history})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "history.append((query, response[\"answer\"]))\n",
    "query = \"List points in it?\"\n",
    "response = conversation({\"question\": query}, {\"chat_history\": history})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "history.append((query, response[\"answer\"]))\n",
    "query = \"What is the aim of it?\"\n",
    "result = conversation({\"question\": query}, {\"chat_history\": history})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506e606-7fef-48d8-b5a7-064c385317b6",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2ce43-d074-4690-b476-737114c01ba1",
   "metadata": {},
   "source": [
    "### Custom sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "38bc987a-25ec-416e-8d74-a8683c8157d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelle/.local/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:370: LifecycleWarning: Model 'mistralai/mixtral-8x7b-instruct-v01' is in deprecated state from 2025-04-30 until 2025-07-30. IDs of alternative models: mistralai/mistral-small-3-1-24b-instruct-2503. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "from pprint import pprint\n",
    "\n",
    "params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 50,  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.MIN_NEW_TOKENS: 10, # this controls the minimum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "        GenParams.TOP_P: 0.2,\n",
    "        GenParams.TOP_K: 1\n",
    "}\n",
    "\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "mixtral_llm = watsonx_model(model_id, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c1f0e2f6-c115-45c4-8f14-51c04c8b4a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/fs0jpct14dqgsbwd5_hk8t740000gn/T/ipykernel_41042/4274201635.py:5: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  location_chain = LLMChain(llm=mixtral_llm, prompt=location_prompt, output_key='dish')\n"
     ]
    }
   ],
   "source": [
    "location_template = \"What is a classic food dish from {location}?\"\n",
    "location_prompt = PromptTemplate(template=location_template, input_variables=['location'])\n",
    "\n",
    "# chain 1\n",
    "location_chain = LLMChain(llm=mixtral_llm, prompt=location_prompt, output_key='dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8178e1bd-4684-4962-bbb0-c1148b06f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_template = \"Given a dish {dish}, give a short and simple recipe on how to make that dish at home.\"\n",
    "dish_prompt = PromptTemplate(template=dish_template, input_variables=['dish'])\n",
    "\n",
    "# chain 2\n",
    "dish_chain = LLMChain(llm=mixtral_llm, prompt=dish_prompt, output_key='recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c0f2f07-6730-48bd-a492-8c8d763514ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_template = \"I have this recipe: {recipe}. Please estimate how much time I need to cook it.\"\n",
    "recipe_prompt = PromptTemplate(template=recipe_template, input_variables=['recipe'])\n",
    "\n",
    "# chain 3\n",
    "recipe_chain = LLMChain(llm=mixtral_llm, prompt=recipe_prompt, output_key='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ebd6d46f-8297-47d6-8e7f-c350a7874bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'dish': '\\n'\n",
      "         '\\n'\n",
      "         'Sauerbraten is a classic German dish that is made by marinating beef '\n",
      "         'in a mixture of vinegar, water, and spices for several days before '\n",
      "         'braising it. The long marinating time tenderizes the meat and gives '\n",
      "         'it',\n",
      " 'location': 'Germany',\n",
      " 'recipe': '\\n'\n",
      "           '\\n'\n",
      "           'Here is a simple recipe for Sauerbraten:\\n'\n",
      "           '\\n'\n",
      "           'Ingredients:\\n'\n",
      "           '\\n'\n",
      "           '* 3-4 pounds beef roast (such as a rump roast or round roast)\\n'\n",
      "           '* 2 cups red',\n",
      " 'time': '\\n'\n",
      "         '\\n'\n",
      "         'I am not sure how long to cook the Sauerbraten. Can you please give '\n",
      "         'me an estimate based on the ingredients and recipe provided?\\n'\n",
      "         '\\n'\n",
      "         'Based on the ingredients and recipe provided, it is difficult to '\n",
      "         'give an exact cooking time'}\n"
     ]
    }
   ],
   "source": [
    "# overall chain\n",
    "\n",
    "overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],\n",
    "                                      input_variables=['location'],\n",
    "                                      output_variables=['dish', 'recipe', 'time'],\n",
    "                                      verbose= True)\n",
    "\n",
    "location_input = {'location':'Germany'}\n",
    "response=overall_chain.invoke(location_input)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eee87-2eee-4100-886d-0204672e4fae",
   "metadata": {},
   "source": [
    "### Pre-defined chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6cfe1-eab3-48bd-9613-f9468b278e8c",
   "metadata": {},
   "source": [
    "#### RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f03d5d9-d46b-41f0-ae12-d7b185a2d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is this paper discussing?', 'result': \" Based on the provided context, it appears that this text is discussing the process of adding and replacing entries within a vector store. However, there is no specific paper mentioned, so it's unclear which paper the question is referring to.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever=vector_store_chroma_pdf.as_retriever()\n",
    "chain = RetrievalQA.from_chain_type(llm=mixtral_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=retriever, \n",
    "                                 return_source_documents=False)\n",
    "query = \"what is this paper discussing?\"\n",
    "search_result=chain.invoke(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c2c9c-ebbf-4f23-8b1d-9cae38abb6b9",
   "metadata": {},
   "source": [
    "#### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c55a472-00d0-48f5-b978-7bb00d75e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This review article discusses the use of machine learning (ML) classifiers in medical image analysis, with a focus on the distinction between shallow learning (SL) and deep learning (DL) algorithms. SL algorithms, such as support vector machines\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import summarize\n",
    "\n",
    "chain = summarize.load_summarize_chain(llm=mixtral_llm, chain_type=\"stuff\", verbose=False)\n",
    "response = chain.invoke(document_pdf)\n",
    "\n",
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7960b3-75e9-4ac6-abad-27d762de14e0",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ad20ccd1-15bf-47f0-835e-e75e33607938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run(\"a = 3; b = 1; print(a+b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26b93e-a8a9-46a3-a336-430657bd5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template is taken from the langchain hub\n",
    "base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "# Create the toolkit\n",
    "toolkit = [PythonREPLTool()]\n",
    "\n",
    "agent = create_react_agent(mixtral_llm, toolkit, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True, handle_parsing_errors=True)  # tools were defined in the toolkit part above\n",
    "\n",
    "response = agent_executor.invoke(input = {\"input\": \"What is the 3rd fibonacci number?\"})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf013c33-2434-48aa-8714-4c93bfc5a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv\"\n",
    ")\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    mixtral_llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    max_iterations=15 # Increase this value\n",
    ")\n",
    "\n",
    "response = agent.invoke(\"How many rows in the dataframe?\",handle_parsing_errors=True)\n",
    "\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-lab-kernel",
   "language": "python",
   "name": "jupyter-lab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
